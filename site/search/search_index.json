{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Vantage","text":"<p>The essence of mathematical insight often comes from finding a new vantage point.</p> <p>Vantage is a Python project designed to build and manage a mathematical knowledge base (KB). It leverages Lean 4 for formal verification, integrates with Large Language Models (specifically Google's Gemini) for potential code/description generation, and uses an SQLite database for persistent storage.</p> <p>Heads Up! This project is currently under active development and is being built day-by-day. Features, documentation, and the overall structure may change frequently. Your patience and feedback are appreciated!</p> <p>This documentation provides detailed information on setting up, configuring, and using the Lean Automator.</p>"},{"location":"#main-sections","title":"Main Sections","text":"<ul> <li>Installation &amp; Setup: How to get the project running, including dependencies and shared library initialization.</li> <li>Configuration: Details on environment variables and other settings.</li> <li>Usage: Examples of how to interact with the knowledge base, LLM, and Lean processor.</li> <li>Testing: Information on running the project's test suite.</li> <li>Project Structure: An overview of the repository layout.</li> <li>Contributing: Guidelines for contributing to the project.</li> </ul>"},{"location":"#get-involved","title":"Get Involved!","text":"<p>As Vantage is actively being built, contributions are highly welcome! If you find this project interesting and would like to help shape its future, please:</p> <ol> <li>Review the Contributing Guidelines for information on how to get started.</li> <li>Feel free to reach out directly via email at <code>justinchadwickasher@gmail.com</code> with any questions, ideas, or offers to help.</li> </ol> <p>For the source code repository, visit GitHub.</p>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at justinchadwickasher@gmail.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>The application uses environment variables, typically loaded from a <code>.env</code> file in the project root directory using a library like <code>python-dotenv</code>.</p>"},{"location":"configuration/#required-variables","title":"Required Variables","text":"<p>These variables must be set for the application to function correctly.</p> <ul> <li> <p><code>GEMINI_API_KEY</code></p> <ul> <li>Description: Your API key for Google AI (Gemini). Obtain one from Google AI Studio.</li> <li>Example Value: <code>YOUR_API_KEY_HERE</code> (Do not commit your actual key to version control)</li> </ul> </li> <li> <p><code>DEFAULT_GEMINI_MODEL</code></p> <ul> <li>Description: The default Google Gemini model name to use for generation tasks.</li> <li>Example Value: <code>gemini-1.5-flash-latest</code> or <code>gemini-2.0-flash</code></li> </ul> </li> <li> <p><code>LEAN_AUTOMATOR_SHARED_LIB_PATH</code></p> <ul> <li>Description: The path (absolute path recommended) to the directory containing the persistent shared Lean library project (e.g., <code>vantage_lib</code>). This must match the directory created and configured during the Installation &amp; Setup.</li> <li>Example Value: <code>/path/to/your/project/vantage_lib</code></li> </ul> </li> </ul>"},{"location":"configuration/#optional-variables","title":"Optional Variables","text":"<p>These variables have default values or are only needed for specific features. You can override them in your <code>.env</code> file.</p> <ul> <li> <p><code>LEAN_AUTOMATOR_SHARED_LIB_MODULE_NAME</code></p> <ul> <li>Description: The Lean module name of the shared library, as defined in its <code>lakefile.toml</code> (e.g., <code>name = \"VantageLib\"</code>). Should match the configuration in the setup guide.</li> <li>Default: <code>VantageLib</code> (Note: This should match the <code>name</code> configured in the shared library's <code>lakefile.toml</code> during setup)</li> </ul> </li> <li> <p><code>DEFAULT_EMBEDDING_MODEL</code></p> <ul> <li>Description: The model used for generating vector embeddings for knowledge base items (used for similarity search). Required if using embedding features.</li> <li>Example Value: <code>text-embedding-004</code></li> </ul> </li> <li> <p><code>KB_DB_PATH</code></p> <ul> <li>Description: Path and filename for the SQLite database storing the knowledge base.</li> <li>Default: <code>knowledge_base.sqlite</code></li> </ul> </li> <li> <p><code>GEMINI_MODEL_COSTS</code></p> <ul> <li>Description: JSON string defining the estimated cost per million units (e.g., tokens) for input and output for specific models. Used for cost tracking. Verify units and costs on the model provider's pricing page.</li> <li>Default: <code>{}</code></li> <li>Example Format: <code>'{\"model-name\": {\"input\": cost_input, \"output\": cost_output}}'</code></li> </ul> </li> <li> <p><code>GEMINI_MAX_RETRIES</code></p> <ul> <li>Description: Maximum number of times to retry a failed API call to the Gemini model.</li> <li>Default: <code>3</code></li> </ul> </li> <li> <p><code>GEMINI_BACKOFF_FACTOR</code></p> <ul> <li>Description: Factor determining the delay between retries (exponential backoff). A factor of 1.0 means delays of 1s, 2s, 4s, etc.</li> <li>Default: <code>1.0</code></li> </ul> </li> <li> <p><code>LEAN_STATEMENT_MAX_ATTEMPTS</code></p> <ul> <li>Description: Maximum attempts the system will make to generate a valid Lean statement using the LLM (if applicable to the workflow).</li> <li>Default: <code>2</code></li> </ul> </li> <li> <p><code>LEAN_PROOF_MAX_ATTEMPTS</code></p> <ul> <li>Description: Maximum attempts the system will make to generate a valid Lean proof using the LLM (if applicable to the workflow).</li> <li>Default: <code>3</code></li> </ul> </li> <li> <p><code>LATEX_MAX_REVIEW_CYCLES</code></p> <ul> <li>Description: Maximum number of review cycles involving the LLM for generating or validating LaTeX output (if applicable to the workflow).</li> <li>Example Value: <code>3</code> (No hard default specified, depends on implementation)</li> </ul> </li> <li> <p><code>LEAN_AUTOMATOR_LAKE_CACHE</code></p> <ul> <li>Description: Optional path to a directory for caching external Lake dependencies, potentially speeding up builds if the shared library uses external Lean libraries.</li> <li>Default: Not set (Lake uses its own default cache location).</li> <li>Example Value: <code>.lake_cache</code></li> </ul> </li> </ul>"},{"location":"configuration/#example-env-file","title":"Example <code>.env</code> File","text":"<p>Here is an example <code>.env</code> file structure incorporating some common settings and overrides, based on the variables described above. Remember to replace placeholders with your actual values and keep sensitive information private.</p> <pre><code># Environment variables for the Lean Automator project\n\n# --- Database ---\n# KB_DB_PATH=knowledge_base.sqlite # Using default\n\n# --- LLM Configuration (Google Gemini) ---\n# IMPORTANT: Keep your API key secure! Do not commit it.\nGEMINI_API_KEY=YOUR_API_KEY_HERE\nDEFAULT_GEMINI_MODEL=gemini-2.0-flash\n\n# --- Vector Embedding Configuration ---\n# Required if using embedding/search features\nDEFAULT_EMBEDDING_MODEL=text-embedding-004\n\n# --- Lean/Lake Configuration ---\n# Use the absolute path to your shared library directory\nLEAN_AUTOMATOR_SHARED_LIB_PATH=/path/to/your/project/vantage_lib\n# Ensure this matches the 'name' in vantage_lib/lakefile.toml\nLEAN_AUTOMATOR_SHARED_LIB_MODULE_NAME=VantageLib\n\n# --- Cost Tracking ---\n# Format: '{\"model-name\": {\"input\": cost_per_million_input_units, \"output\": cost_per_million_output_units}}'\n# Costs are examples, verify current pricing. Units might be tokens/chars.\nGEMINI_MODEL_COSTS='{\"gemini-2.0-flash\": {\"input\": 0.1, \"output\": 0.4}, \"models/text-embedding-004\": {\"input\": 0.0, \"output\": 0.0}}'\n\n# --- LLM Retry/Backoff Overrides (Optional) ---\nGEMINI_MAX_RETRIES=5\nGEMINI_BACKOFF_FACTOR=1.5\n\n# --- LaTeX Generation (Optional) ---\nLATEX_MAX_REVIEW_CYCLES=3\n\n# --- Other Optional Settings ---\n# LEAN_STATEMENT_MAX_ATTEMPTS=2 # Using default\n# LEAN_PROOF_MAX_ATTEMPTS=3 # Using default\n# LEAN_AUTOMATOR_LAKE_CACHE=.lake_cache # Using default Lake cache\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>We're excited that you're interested in contributing to this project! Collaboration is welcome and encouraged. Whether it's reporting a bug, discussing improvements, or submitting code changes, your help is valued.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please note that this project is released with a Contributor Code of Conduct. By participating in this project, you agree to abide by its terms, available in the code of conduct file.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<p>We use GitHub Issues to track bugs, feature requests, documentation updates, and other tasks. We also use a GitHub Project board to visualize our workflow, plan upcoming work, and track progress on higher-level objectives.</p> <ol> <li>Find Work to Do:<ul> <li>Issues Tab: Browse individual tasks directly on the Issues tab. This is the primary list of all reported bugs, requested features, etc.</li> <li>Project Board: Get a visual overview of our workflow, priorities, and what's currently in progress on our Project Board. The board organizes many of the items from the Issues tab into different stages (like Todo, In Progress, Done) and helps show our planned roadmap.</li> </ul> </li> <li>Look for Labels: Whether viewing the Issues tab or the Project Board (where labels are often visible on cards), keep an eye out for:<ul> <li><code>good first issue</code>: Specifically chosen as good starting points if you're new.</li> <li><code>help wanted</code>: Issues specifically marked as needing contributor help.</li> <li>Other labels like <code>bug</code>, <code>enhancement</code>, <code>documentation</code> describe the type of work. (Tip: You can filter the issues list and sometimes the project board by labels!)</li> </ul> </li> <li>Write Clear Issues (If Creating New Ones): If you're reporting a bug or suggesting a feature not already tracked, please create a new issue on the Issues tab and be as clear as possible. For bugs, include steps to reproduce, expected results, and actual results.</li> <li>Claim an Issue: If you find an existing issue you'd like to work on (either via the Issues tab or the Project Board), please navigate to the issue itself and leave a comment (e.g., \"I'd like to work on this\"). This helps avoid duplicated effort. A maintainer can then assign the issue to you.</li> <li>Work on Your Contribution:<ul> <li>Fork the repository to your own GitHub account.</li> <li>Create a new branch for your changes (e.g., <code>git checkout -b feature/add-new-widget</code> or <code>git checkout -b fix/login-bug</code>).</li> <li>Make your changes, following any coding style guidelines mentioned (you might want to add a section on this later!).</li> <li>Commit your changes with clear commit messages.</li> </ul> </li> <li>Submit a Pull Request (PR):<ul> <li>Push your branch to your fork on GitHub.</li> <li>Open a Pull Request back to the main repository (<code>justincasher/vantage</code>).</li> <li>In the PR description, please link the issue you're addressing by including text like \"Closes #123\" or \"Fixes #123\" (replace <code>123</code> with the actual issue number). This automatically links the PR to the issue and helps project tracking.</li> <li>Provide a clear description of the changes you've made.</li> </ul> </li> </ol>"},{"location":"contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<p>Please refer to the installation.md file for detailed instructions on how to clone the repository, install dependencies, and set up your local development environment.</p>"},{"location":"contributing/#questions-or-broader-discussions","title":"Questions or Broader Discussions?","text":"<ul> <li>For questions related to a specific issue or Pull Request, please use the comment section directly on GitHub within that issue/PR.</li> <li>For broader ideas, strategic discussions about the project's direction (which you might see reflected on the Project Board), or if you're unsure where to start and want to connect with the project lead, feel free to email Justin Asher directly at justinchadwickasher@gmail.com.</li> </ul> <p>Thank you for considering contributing!</p>"},{"location":"installation/","title":"Installation &amp; Setup","text":"<p>Follow these steps to get the Lean Automator project running.</p> <p>The shared library initialization (Steps 5-10) only needs to be done once. The <code>lean_processor</code> will automatically add new <code>.lean</code> files to the library's source directory (e.g., <code>vantage_lib/VantageLib/</code>) and trigger builds within the shared library directory later.</p> <ol> <li> <p>Clone the repository: <pre><code>git clone &lt;your-repository-url&gt;\ncd &lt;repository-directory-name&gt; # e.g., cd vantage\n</code></pre></p> </li> <li> <p>Create and activate a virtual environment (recommended): <pre><code># On macOS/Linux\npython3 -m venv venv\nsource venv/bin/activate\n\n# On Windows\npython -m venv venv\n.\\venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install Python dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Set up Environment Variables:     Create a <code>.env</code> file in the project root directory (e.g., <code>vantage/.env</code>). Add your API key and placeholders for library configuration initially. You will confirm the exact values in later steps.     <pre><code># Required:\nGEMINI_API_KEY=YOUR_GEMINI_KEY_HERE\nDEFAULT_GEMINI_MODEL=gemini-1.5-flash-latest # Or your preferred model\n\n# --- Shared Library Configuration (Confirm values in steps below) ---\n# Path (absolute recommended) to the directory created in Step 5\nLEAN_AUTOMATOR_SHARED_LIB_PATH=/replace/with/absolute/path/to/vantage_lib\n# The Lean library module name defined inside the shared library project (Step 7)\nLEAN_AUTOMATOR_SHARED_LIB_MODULE_NAME=VantageLib # Default if using steps below\n\n# Optional (Defaults shown):\n# GEMINI_MODEL_COSTS='{\"gemini-1.5-flash-latest\": {\"input\": 0.35, \"output\": 0.70}}'\n# KB_DB_PATH=knowledge_base.sqlite\n# GEMINI_MAX_RETRIES=3\n# GEMINI_BACKOFF_FACTOR=1.0\n# LEAN_STATEMENT_MAX_ATTEMPTS=2\n# LEAN_PROOF_MAX_ATTEMPTS=3\n# LEAN_AUTOMATOR_LAKE_CACHE=.lake_cache # Optional: For external Lake deps cache\n</code></pre></p> <ul> <li>Replace <code>YOUR_GEMINI_KEY_HERE</code> with your actual key.</li> <li>You will set the correct absolute path for <code>LEAN_AUTOMATOR_SHARED_LIB_PATH</code> and confirm <code>LEAN_AUTOMATOR_SHARED_LIB_MODULE_NAME</code> in Step 10.</li> </ul> </li> <li> <p>Create Shared Library Directory:     This directory will hold the Lake project for your persistent Lean library. Create it and navigate into it from your project root (e.g., <code>vantage/</code>).     <pre><code># Use a consistent name (e.g., vantage_lib)\n# (If it already exists from a previous attempt, ensure it's empty first)\nmkdir vantage_lib\ncd vantage_lib\n</code></pre></p> </li> <li> <p>Initialize Shared Library with Lake:     Initialize a default Lake project within the new directory. On older Lake versions using <code>lakefile.toml</code>, this often configures both a library (e.g., <code>VantageLib</code>) and an executable (<code>Main.lean</code>), deriving names from the directory.     <pre><code># Initialize default Lake project in the current directory (e.g., vantage_lib/)\nlake init .\n</code></pre></p> </li> <li> <p>Configure Shared Library (<code>lakefile.toml</code>):     Manually edit the generated <code>lakefile.toml</code> to configure the project as library-only.</p> <ul> <li>Open <code>lakefile.toml</code>.</li> <li>Find the library definition section (e.g., <code>[[lean_lib]]</code>). Ensure the <code>name</code> attribute matches your desired module name (e.g., <code>name = \"VantageLib\"</code>). Keep this section. This name must match the <code>LEAN_AUTOMATOR_SHARED_LIB_MODULE_NAME</code> environment variable.</li> <li>Find the executable definition section (e.g., <code>[[lean_exe]]</code>). Delete this entire section (usually 3 lines).</li> <li>(Optional but Recommended) Find the <code>defaultTargets = [...]</code> line near the top and delete it to avoid potential build issues.</li> <li>Save the changes to <code>lakefile.toml</code>.</li> </ul> </li> <li> <p>Clean Up Shared Library Files:     Delete the unnecessary files generated by the default <code>lake init</code> associated with the executable or placeholders. Run these commands while still inside the shared library directory (e.g., <code>vantage_lib/</code>).     <pre><code># Remove executable entry point and potential library root file:\nrm -f Main.lean VantageLib.lean\n\n# Remove default placeholder inside the library source directory:\n# (Replace VantageLib if you configured a different library name in Step 7)\nrm -f VantageLib/Basic.lean\n\n# Optional: remove default readme:\nrm -f README.md\n</code></pre></p> </li> <li> <p>Optional: Build Check Shared Library:     Verify the Lake configuration by running a build command. This ensures the <code>lakefile.toml</code> and directory structure are valid. Run this while still inside the shared library directory (e.g., <code>vantage_lib/</code>).     <pre><code>lake build\n</code></pre> Note: If this fails with <code>error: package 'vantage_lib' has no target 'vantage_lib'</code>, double-check that you deleted the <code>defaultTargets = [\"vantage_lib\"]</code> line from your <code>lakefile.toml</code> in Step 7. A successful command indicates the setup is correct so far.</p> </li> <li> <p>Finalize Setup:     Navigate back to the project root directory and ensure your environment variables are correctly set.     <pre><code># Navigate back to the project root (e.g., vantage/)\ncd ..\n</code></pre></p> <ul> <li>CRITICAL: Now, update your <code>.env</code> file (from Step 4) or export the environment variables:<ul> <li>Set <code>LEAN_AUTOMATOR_SHARED_LIB_PATH</code> to the absolute path of the shared library directory you created (e.g., the output of <code>pwd</code> when you were inside <code>vantage_lib</code>).</li> <li>Confirm <code>LEAN_AUTOMATOR_SHARED_LIB_MODULE_NAME</code> matches the library name you configured in Step 7 (e.g., <code>VantageLib</code>).</li> </ul> </li> </ul> </li> </ol>"},{"location":"project_structure/","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 pytest.ini\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 scripts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test_latex_processor_basic.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test_lean_processor_basic.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 test_lean_processor_tree.py\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lean_automator\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kb_search.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kb_storage.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 latex_processor.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 lean_interaction.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 lean_processor.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 lean_proof_repair.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 llm_call.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 vantage.egg-info\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 PKG-INFO\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 SOURCES.txt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dependency_links.txt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 top_level.txt\n\u251c\u2500\u2500 tests\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test_kb_search_integration.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test_kb_storage_integration.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test_lean_interaction_integration.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test_llm_call_integration.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 unit\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 test_kb_search_unit.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 test_kb_storage_unit.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 test_lean_interaction_unit.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 test_llm_call_unit.py\n\u2514\u2500\u2500 vantage_lib\n    \u251c\u2500\u2500 VantageLib\n    \u251c\u2500\u2500 lake-manifest.json\n    \u251c\u2500\u2500 lakefile.toml\n    \u2514\u2500\u2500 lean-toolchain\n</code></pre> <p>(Note: <code>.sqlite</code> database files and <code>.env</code> are typically generated/created in the root but omitted from the source structure example)</p>"},{"location":"testing/","title":"Testing","text":"<p>The project includes unit and integration tests using <code>pytest</code>.</p> <ol> <li> <p>Install test dependencies: <pre><code>pip install pytest pytest-asyncio\n</code></pre></p> </li> <li> <p>Run tests:     Navigate to the project root directory (where <code>pytest.ini</code> is located).</p> <ul> <li> <p>Run all tests: <pre><code>pytest\n</code></pre></p> <ul> <li>Note: Running all tests includes integration tests... (rest of note) ...</li> </ul> </li> <li> <p>Run only unit tests (excluding integration tests): <pre><code>pytest -m \"not integration\"\n</code></pre></p> </li> <li> <p>Run only integration tests: <pre><code>pytest -m integration\n</code></pre></p> </li> <li> <p>Exclude slow tests: <pre><code>pytest -m \"not slow\"\n</code></pre></p> </li> </ul> <p>You can combine markers as needed (e.g., <code>pytest -m \"integration and not slow\"</code>)</p> </li> </ol>"},{"location":"usage/","title":"Lean Automator: Usage Examples","text":"<p>This document provides examples demonstrating the core workflow of the Lean Automator system. It aims to give a practical overview of how the different modules (<code>kb_storage</code>, <code>llm_call</code>, <code>latex_processor</code>, <code>lean_processor</code>, <code>kb_search</code>) interact to build and manage the mathematical knowledge base.</p> <p>Prerequisites:</p> <ul> <li>Python environment with necessary libraries installed (<code>google-generativeai</code>, <code>numpy</code>, etc.).</li> <li>Lean 4 and Lake installed.</li> <li>Environment variables set:<ul> <li><code>GEMINI_API_KEY</code>: Your Google AI API key.</li> <li><code>DEFAULT_GEMINI_MODEL</code>: e.g., <code>gemini-1.5-flash-latest</code>.</li> <li><code>DEFAULT_EMBEDDING_MODEL</code>: e.g., <code>text-embedding-004</code> (will be prefixed with <code>models/</code>).</li> <li><code>GEMINI_MODEL_COSTS</code>: JSON string with model costs per million units (optional, for cost tracking).</li> <li><code>KB_DB_PATH</code>: Path to the SQLite database file (e.g., <code>knowledge_base.sqlite</code>). Defaults if not set.</li> <li><code>LEAN_AUTOMATOR_SHARED_LIB_PATH</code>: Absolute path to your persistent shared Lean library directory.</li> <li><code>LEAN_AUTOMATOR_SHARED_LIB_MODULE_NAME</code>: The name of your shared Lean library module (e.g., <code>ProvenKB</code>).</li> </ul> </li> <li>The shared Lean library at <code>LEAN_AUTOMATOR_SHARED_LIB_PATH</code> should be initialized (<code>lake init &lt;YourModuleName&gt;</code>).</li> </ul> <p>(These examples assume async execution using <code>asyncio.run()</code>)</p>"},{"location":"usage/#1-initialization","title":"1. Initialization","text":"<p>First, ensure the database schema exists and initialize the Gemini client.</p> <pre><code>import asyncio\nfrom lean_automator import kb_storage, llm_call\n\n# Initialize the database (creates tables if they don't exist)\n# Uses KB_DB_PATH env var or default 'knowledge_base.sqlite'\nprint(\"Initializing database...\")\nkb_storage.initialize_database()\nprint(\"Database initialized.\")\n\n# Initialize the Gemini Client\n# Reads API key, model names, retry settings from environment variables\ntry:\n    print(\"Initializing Gemini client...\")\n    client = llm_call.GeminiClient()\n    print(\"Gemini client initialized.\")\n    print(f\"Using Generation Model: {client.default_generation_model}\")\n    print(f\"Using Embedding Model: {client.default_embedding_model}\")\nexcept Exception as e:\n    print(f\"FATAL: Failed to initialize Gemini Client: {e}\")\n    # Cannot proceed without the client\n    exit()\n</code></pre>"},{"location":"usage/#2-creating-and-saving-a-basic-kb-item-definition","title":"2. Creating and Saving a Basic KB Item (Definition)","text":"<p>Let's create a simple definition and save it to the knowledge base.</p> <pre><code># Continued from previous block...\nfrom lean_automator.kb_storage import KBItem, ItemType, ItemStatus, save_kb_item\n\nasync def add_definition():\n    print(\"\\nCreating a new definition KBItem...\")\n    definition_item = KBItem(\n        unique_name=\"MyProject.BasicDefs.ZeroIsNatural\", # Use Lean-like naming convention\n        item_type=ItemType.DEFINITION,\n        description_nl=\"Defines that the number zero is considered a natural number in our system.\",\n        topic=\"MyProject.BasicDefs\",\n        # No LaTeX or Lean code initially\n        status=ItemStatus.PENDING # Start as Pending\n    )\n\n    print(f\"Saving item '{definition_item.unique_name}'...\")\n    try:\n        # save_kb_item is async and handles INSERT or UPDATE.\n        # If description_nl or latex_statement changes and client is provided,\n        # it can automatically trigger embedding generation.\n        saved_item = await save_kb_item(definition_item, client=client)\n        print(f\"Item saved successfully with ID: {saved_item.id} and status: {saved_item.status.name}\")\n        # If description_nl was present, embedding_nl might now be populated (as bytes)\n        print(f\"Embedding NL present: {saved_item.embedding_nl is not None}\")\n        return saved_item.unique_name # Return name for dependency use\n    except Exception as e:\n        print(f\"Error saving item: {e}\")\n        return None\n\n# Run the async function\ndefinition_name = asyncio.run(add_definition())\n</code></pre>"},{"location":"usage/#3-adding-a-dependent-item-theorem","title":"3. Adding a Dependent Item (Theorem)","text":"<p>Now, add a theorem that depends on the definition created above.</p> <pre><code># Continued from previous block...\nasync def add_theorem(dependency_name: str):\n    if not dependency_name:\n        print(\"\\nSkipping theorem creation, definition dependency failed.\")\n        return None\n\n    print(\"\\nCreating a new theorem KBItem...\")\n    theorem_item = KBItem(\n        unique_name=\"MyProject.Theorems.ZeroProperty\",\n        item_type=ItemType.THEOREM,\n        description_nl=\"States a simple property involving the natural number zero, based on its definition.\",\n        topic=\"MyProject.Theorems\",\n        plan_dependencies=[dependency_name], # List the unique_name of the definition\n        status=ItemStatus.PENDING # Start as Pending, needs processing\n    )\n\n    print(f\"Saving item '{theorem_item.unique_name}'...\")\n    try:\n        saved_item = await save_kb_item(theorem_item, client=client)\n        print(f\"Item saved successfully with ID: {saved_item.id} and status: {saved_item.status.name}\")\n        print(f\"Plan dependencies: {saved_item.plan_dependencies}\")\n        print(f\"Embedding NL present: {saved_item.embedding_nl is not None}\")\n        return saved_item.unique_name\n    except Exception as e:\n        print(f\"Error saving item: {e}\")\n        return None\n\n# Run the async function, passing the name of the definition\ntheorem_name = asyncio.run(add_theorem(definition_name))\n</code></pre>"},{"location":"usage/#4-processing-latex","title":"4. Processing LaTeX","text":"<p>Items often start with just a description. The <code>latex_processor</code> uses the LLM to generate and review the LaTeX statement and proof (if required).</p> <pre><code># Continued from previous block...\nfrom lean_automator import latex_processor\n\nasync def process_latex(item_name: str):\n    if not item_name:\n        print(\"\\nSkipping LaTeX processing, theorem creation failed.\")\n        return False\n\n    print(f\"\\nProcessing LaTeX for item '{item_name}'...\")\n    # This function calls the LLM multiple times (generate, review, potentially refine).\n    # It updates the item in the database.\n    # Assumes the dependency 'MyProject.BasicDefs.ZeroIsNatural' has acceptable LaTeX (or is simple enough).\n    # For this example, let's assume the definition doesn't need complex LaTeX processing first.\n    try:\n        success = await latex_processor.generate_and_review_latex(\n            unique_name=item_name,\n            client=client\n            # db_path=... # Optional override\n        )\n\n        if success:\n            print(f\"LaTeX processing successful for {item_name}.\")\n            # Check the item status in DB - should be LATEX_ACCEPTED\n            item_after_latex = kb_storage.get_kb_item_by_name(item_name)\n            if item_after_latex:\n                print(f\"New status: {item_after_latex.status.name}\")\n                print(f\"LaTeX Statement present: {item_after_latex.latex_statement is not None}\")\n                print(f\"LaTeX Proof present: {item_after_latex.latex_proof is not None}\")\n                # Saving the accepted statement triggered embedding generation\n                print(f\"Embedding LaTeX present: {item_after_latex.embedding_latex is not None}\")\n            return True\n        else:\n            print(f\"LaTeX processing failed for {item_name}. Check logs and DB status/feedback.\")\n            # Status might be LATEX_REJECTED_FINAL or ERROR\n            item_after_latex = kb_storage.get_kb_item_by_name(item_name)\n            if item_after_latex:\n                 print(f\"Final status: {item_after_latex.status.name}\")\n                 print(f\"Review Feedback: {item_after_latex.latex_review_feedback}\")\n            return False\n    except Exception as e:\n        print(f\"Error during LaTeX processing call: {e}\")\n        return False\n\n# Run the async function\nlatex_success = asyncio.run(process_latex(theorem_name))\n</code></pre>"},{"location":"usage/#5-processing-lean-code","title":"5. Processing Lean Code","text":"<p>Once LaTeX is accepted (<code>LATEX_ACCEPTED</code>), the <code>lean_processor</code> can generate and verify the Lean 4 code.</p> <pre><code># Continued from previous block...\nfrom lean_automator import lean_processor\n\nasync def process_lean(item_name: str, previous_step_success: bool):\n    if not item_name or not previous_step_success:\n        print(\"\\nSkipping Lean processing due to previous step failure or missing item.\")\n        return False\n\n    # Fetch item to ensure it's in the correct state\n    item = kb_storage.get_kb_item_by_name(item_name)\n    if not item or item.status != ItemStatus.LATEX_ACCEPTED:\n        print(f\"\\nSkipping Lean processing for {item_name}. Expected LATEX_ACCEPTED status, found {item.status.name if item else 'MISSING'}.\")\n        return False\n\n    print(f\"\\nProcessing Lean code for item '{item_name}'...\")\n    # This function involves LLM calls (statement gen, proof gen) and\n    # calls lean_interaction.check_and_compile_item, which uses 'lake'\n    # and interacts with the persistent shared library.\n    # Assumes the dependency ('MyProject.BasicDefs.ZeroIsNatural') has been\n    # successfully processed to PROVEN status and exists in the shared library.\n    # For this demo, we'll assume that happened somehow (e.g., manual addition or prior run).\n    try:\n        # Make sure the dependency is marked PROVEN conceptually for this to work\n        # In a real run, you'd process dependencies first.\n        print(\"INFO: Assuming dependency 'MyProject.BasicDefs.ZeroIsNatural' is PROVEN in the shared library.\")\n\n        success = await lean_processor.generate_and_verify_lean(\n            unique_name=item_name,\n            client=client\n            # db_path=..., # Optional override\n            # lake_executable_path=..., # Optional override\n            # timeout_seconds=... # Optional override\n        )\n\n        # Check final status\n        item_after_lean = kb_storage.get_kb_item_by_name(item_name)\n        status_after = item_after_lean.status.name if item_after_lean else \"MISSING\"\n\n        if success:\n            print(f\"Lean processing successful for {item_name}. Final Status: {status_after}\")\n            # Item status should be PROVEN, and code added to the shared library.\n            print(f\"Lean code present: {item_after_lean.lean_code is not None and 'sorry' not in item_after_lean.lean_code}\")\n            return True\n        else:\n            print(f\"Lean processing failed for {item_name}. Final Status: {status_after}\")\n            # Status might be LEAN_VALIDATION_FAILED or ERROR. Check lean_error_log.\n            print(f\"Lean Error Log present: {item_after_lean.lean_error_log is not None}\")\n            # print(f\"Error Log Snippet: {item_after_lean.lean_error_log[:500] if item_after_lean.lean_error_log else 'N/A'}\")\n            return False\n    except Exception as e:\n        print(f\"Error during Lean processing call: {e}\")\n        # Manually check DB status if needed\n        item_after_crash = kb_storage.get_kb_item_by_name(item_name)\n        status_after = item_after_crash.status.name if item_after_crash else \"MISSING\"\n        print(f\"Status after crash: {status_after}\")\n        return False\n\n# Run the async function\nlean_success = asyncio.run(process_lean(theorem_name, latex_success))\n</code></pre>"},{"location":"usage/#6-semantic-search","title":"6. Semantic Search","text":"<p>After processing, items have embeddings (from description or LaTeX statement). We can search for items related to a query.</p> <pre><code># Continued from previous block...\nfrom lean_automator import kb_search\n\nasync def perform_search(query: str):\n    if not query:\n        print(\"\\nSkipping search, no query provided.\")\n        return\n\n    print(f\"\\nPerforming semantic search for: '{query}'\")\n    try:\n        # Search against the natural language description embeddings ('nl')\n        # Alternatively, use 'latex' to search against latex_statement embeddings\n        similar_items = await kb_search.find_similar_items(\n            query_text=query,\n            search_field='nl', # 'nl' or 'latex'\n            client=client,\n            top_n=3\n            # db_path=... # Optional override\n        )\n\n        if similar_items:\n            print(\"Found similar items:\")\n            for item, score in similar_items:\n                print(f\"- {item.unique_name} (Type: {item.item_type.name}, Status: {item.status.name}) - Score: {score:.4f}\")\n        else:\n            print(\"No similar items found.\")\n\n    except Exception as e:\n        print(f\"Error during semantic search: {e}\")\n\n# Construct a query related to the theorem we added\nsearch_query = \"natural number zero property\"\nif theorem_name: # Only search if the theorem was likely created\n    asyncio.run(perform_search(search_query))\nelse:\n     asyncio.run(perform_search(\"Example search query about mathematics\"))\n</code></pre>"},{"location":"usage/#7-direct-embedding-generation","title":"7. Direct Embedding Generation","text":"<p>You can also generate embeddings directly using the client.</p> <pre><code># Continued from previous block...\nasync def generate_single_embedding():\n    print(\"\\nGenerating a direct embedding...\")\n    text_to_embed = \"Cosine similarity measures the cosine of the angle between two non-zero vectors.\"\n    task = \"SEMANTIC_SIMILARITY\" # Or RETRIEVAL_QUERY, RETRIEVAL_DOCUMENT etc.\n    try:\n        embeddings = await client.embed_content(\n            contents=text_to_embed,\n            task_type=task\n        )\n        if embeddings:\n            print(f\"Generated embedding for task '{task}'. Vector dimensions: {len(embeddings[0])}\")\n            # print(f\"Embedding vector (first 10 dims): {embeddings[0][:10]}\")\n        else:\n            print(\"Embedding generation returned empty result.\")\n    except Exception as e:\n        print(f\"Error generating direct embedding: {e}\")\n\nasyncio.run(generate_single_embedding())\n</code></pre>"},{"location":"usage/#8-retrieving-cost-summary","title":"8. Retrieving Cost Summary","text":"<p>Track estimated API costs using the cost tracker integrated into the <code>GeminiClient</code>.</p> <pre><code># Continued from previous block...\n\nprint(\"\\nRetrieving API Cost Summary...\")\n# The client's cost_tracker accumulates usage across all calls (generate, embed)\nsummary = client.cost_tracker.get_summary()\n\nimport json\nprint(json.dumps(summary, indent=2))\n\n# You can also get just the total\ntotal_cost = client.cost_tracker.get_total_cost()\nprint(f\"\\nEstimated Total Cost: ${total_cost:.6f}\")\n</code></pre> <p>This concludes the basic usage examples, demonstrating the flow from item creation through LaTeX and Lean processing, leveraging LLMs and the Lean compiler, and utilizing semantic search capabilities.</p>"},{"location":"reference/","title":"API Reference Index","text":"<p>This index lists all the modules available in the API reference.</p> <p>Click on a module name below to see its detailed documentation:</p> <ul> <li>Kb Search</li> <li>Kb Storage</li> <li>Latex Processor</li> <li>Lean Interaction</li> <li>Lean Processor</li> <li>Lean Proof Repair</li> <li>Llm Call</li> </ul>"},{"location":"reference/kb_search/","title":"KB Search","text":"<p>Generates embeddings and performs semantic search in the Knowledge Base.</p> <p>This module provides functions for generating text embeddings using a Gemini client and performing semantic search within the Knowledge Base by comparing vector similarity (cosine similarity) between a query embedding and stored embeddings.</p>"},{"location":"reference/kb_search/#lean_automator.kb_search","title":"<code>lean_automator.kb_search</code>","text":""},{"location":"reference/kb_search/#lean_automator.kb_search-classes","title":"Classes","text":""},{"location":"reference/kb_search/#lean_automator.kb_search-functions","title":"Functions","text":""},{"location":"reference/kb_search/#lean_automator.kb_search.generate_embedding","title":"<code>generate_embedding(text: str, task_type: str, client: GeminiClient) -&gt; Optional[np.ndarray]</code>  <code>async</code>","text":"<p>Generates an embedding for the given text using the configured Gemini client.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text content to embed.</p> required <code>task_type</code> <code>str</code> <p>The task type for the embedding (e.g., \"RETRIEVAL_DOCUMENT\").</p> required <code>client</code> <code>GeminiClient</code> <p>An initialized GeminiClient instance.</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>A numpy array representing the embedding, or None if generation fails.</p> Source code in <code>lean_automator/kb_search.py</code> <pre><code>async def generate_embedding(\n    text: str,\n    task_type: str,\n    client: GeminiClient\n) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Generates an embedding for the given text using the configured Gemini client.\n\n    Args:\n        text: The text content to embed.\n        task_type: The task type for the embedding (e.g., \"RETRIEVAL_DOCUMENT\").\n        client: An initialized GeminiClient instance.\n\n    Returns:\n        A numpy array representing the embedding, or None if generation fails.\n    \"\"\"\n    if not client:\n        warnings.warn(\"GeminiClient not available for embedding generation.\")\n        return None\n    if not text:\n        warnings.warn(\"Attempted to generate embedding for empty text.\")\n        return None\n\n    try:\n        # Assumes client.embed_content returns a list of embeddings,\n        # even for single input. We take the first one.\n        embeddings_list = await client.embed_content(contents=text, task_type=task_type)\n        if embeddings_list and embeddings_list[0]:\n            return np.array(embeddings_list[0], dtype=EMBEDDING_DTYPE)\n        else:\n            warnings.warn(f\"Embedding generation returned empty result for task '{task_type}'.\")\n            return None\n    except Exception as e:\n        warnings.warn(f\"Error generating embedding: {e}\")\n        return None\n</code></pre>"},{"location":"reference/kb_search/#lean_automator.kb_search.find_similar_items","title":"<code>find_similar_items(query_text: str, search_field: str, client: GeminiClient, *, task_type_query: str = 'RETRIEVAL_QUERY', db_path: Optional[str] = None, top_n: int = 5) -&gt; List[Tuple[KBItem, float]]</code>  <code>async</code>","text":"<p>Finds KBItems with embeddings similar to the query text.</p> <p>Performs a brute-force cosine similarity search across all items having an embedding for the specified field.</p> <p>Parameters:</p> Name Type Description Default <code>query_text</code> <code>str</code> <p>The natural language query.</p> required <code>search_field</code> <code>str</code> <p>Which embedding to search ('nl' or 'latex').</p> required <code>client</code> <code>GeminiClient</code> <p>An initialized GeminiClient instance.</p> required <code>task_type_query</code> <code>str</code> <p>The task type for embedding the query (default: RETRIEVAL_QUERY).</p> <code>'RETRIEVAL_QUERY'</code> <code>db_path</code> <code>Optional[str]</code> <p>Optional path to the database file.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>The maximum number of similar items to return.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Tuple[KBItem, float]]</code> <p>A list of tuples, each containing a matching KBItem and its similarity</p> <code>List[Tuple[KBItem, float]]</code> <p>score, sorted by score descending. Returns empty list on error or no matches.</p> Source code in <code>lean_automator/kb_search.py</code> <pre><code>async def find_similar_items(\n    query_text: str,\n    search_field: str, # 'nl' or 'latex'\n    client: GeminiClient,\n    *, # Make subsequent arguments keyword-only\n    task_type_query: str = \"RETRIEVAL_QUERY\",\n    db_path: Optional[str] = None,\n    top_n: int = 5\n) -&gt; List[Tuple[KBItem, float]]:\n    \"\"\"\n    Finds KBItems with embeddings similar to the query text.\n\n    Performs a brute-force cosine similarity search across all items\n    having an embedding for the specified field.\n\n    Args:\n        query_text: The natural language query.\n        search_field: Which embedding to search ('nl' or 'latex').\n        client: An initialized GeminiClient instance.\n        task_type_query: The task type for embedding the query (default: RETRIEVAL_QUERY).\n        db_path: Optional path to the database file.\n        top_n: The maximum number of similar items to return.\n\n    Returns:\n        A list of tuples, each containing a matching KBItem and its similarity\n        score, sorted by score descending. Returns empty list on error or no matches.\n    \"\"\"\n    if search_field not in ['nl', 'latex']:\n        raise ValueError(\"search_field must be 'nl' or 'latex'\")\n    if not client:\n        warnings.warn(\"GeminiClient not available for embedding search.\")\n        return []\n    if get_all_items_with_embedding is None or get_kb_item_by_name is None:\n         warnings.warn(\"kb_storage functions not available for search.\")\n         return []\n\n    embedding_column = f\"embedding_{search_field}\"\n    effective_db_path = db_path or DEFAULT_DB_PATH\n\n    # 1. Generate query embedding\n    query_vector = await generate_embedding(query_text, task_type_query, client)\n    if query_vector is None:\n        warnings.warn(f\"Failed to generate query embedding for: '{query_text[:50]}...'\")\n        return []\n\n    # 2. Fetch all existing document embeddings for the target field\n    try:\n        all_embeddings_data = get_all_items_with_embedding(embedding_column, effective_db_path)\n    except Exception as e:\n         warnings.warn(f\"Failed to retrieve embeddings from database: {e}\")\n         return []\n\n    if not all_embeddings_data:\n        print(f\"No items found with embeddings in field '{embedding_column}'.\")\n        return []\n\n    # 3. Calculate similarities\n    similarities: List[Tuple[str, float]] = [] # Store (unique_name, score)\n    for item_id, unique_name, embedding_blob in all_embeddings_data:\n        doc_vector = _bytes_to_vector(embedding_blob)\n        if doc_vector is not None:\n            # Optional: Check dimensions match query_vector here\n            if query_vector.shape == doc_vector.shape:\n                score = _cosine_similarity(query_vector, doc_vector)\n                similarities.append((unique_name, score))\n            else:\n                 warnings.warn(f\"Dimension mismatch for item '{unique_name}': Query={query_vector.shape}, Doc={doc_vector.shape}. Skipping.\")\n        else:\n             warnings.warn(f\"Could not decode embedding for item '{unique_name}' (ID: {item_id}). Skipping.\")\n\n\n    # 4. Sort by similarity (descending)\n    similarities.sort(key=lambda x: x[1], reverse=True)\n\n    # 5. Get top N results and retrieve full KBItem objects\n    top_results: List[Tuple[KBItem, float]] = []\n    for unique_name, score in similarities[:top_n]:\n        item = get_kb_item_by_name(unique_name, effective_db_path)\n        if item:\n            top_results.append((item, score))\n        else:\n            # Should ideally not happen if unique_name came from the DB\n            warnings.warn(f\"Could not retrieve KBItem '{unique_name}' after similarity search.\")\n\n    return top_results\n</code></pre>"},{"location":"reference/kb_storage/","title":"KB Storage","text":"<p>Defines data structures and SQLite storage for a mathematical knowledge base.</p> <p>This module provides the core data structures (<code>KBItem</code>, <code>LatexLink</code>, etc.) representing mathematical concepts and their metadata within a knowledge base. It also includes functions for creating, initializing, saving, and retrieving these items from an SQLite database. The default database path is determined by the <code>KB_DB_PATH</code> environment variable or defaults to 'knowledge_base.sqlite'. Functionality includes handling text embeddings (stored as BLOBs) associated with natural language descriptions and LaTeX statements.</p>"},{"location":"reference/kb_storage/#lean_automator.kb_storage","title":"<code>lean_automator.kb_storage</code>","text":""},{"location":"reference/kb_storage/#lean_automator.kb_storage-classes","title":"Classes","text":""},{"location":"reference/kb_storage/#lean_automator.kb_storage.ItemType","title":"<code>ItemType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates the different types of items stored in the Knowledge Base.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>class ItemType(enum.Enum):\n    \"\"\"Enumerates the different types of items stored in the Knowledge Base.\"\"\"\n    DEFINITION = \"Definition\"; AXIOM = \"Axiom\"; THEOREM = \"Theorem\"; LEMMA = \"Lemma\"; PROPOSITION = \"Proposition\"; COROLLARY = \"Corollary\"; EXAMPLE = \"Example\"; REMARK = \"Remark\"; CONJECTURE = \"Conjecture\"; NOTATION = \"Notation\"; STRUCTURE = \"Structure\"\n\n    def requires_proof(self) -&gt; bool:\n        \"\"\"Checks if this item type typically requires a proof.\"\"\"\n        return self in {\n            ItemType.THEOREM, ItemType.LEMMA, ItemType.PROPOSITION,\n            ItemType.COROLLARY, ItemType.EXAMPLE # Examples might need verification/proof\n        }\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.ItemType-functions","title":"Functions","text":""},{"location":"reference/kb_storage/#lean_automator.kb_storage.ItemType.requires_proof","title":"<code>requires_proof() -&gt; bool</code>","text":"<p>Checks if this item type typically requires a proof.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def requires_proof(self) -&gt; bool:\n    \"\"\"Checks if this item type typically requires a proof.\"\"\"\n    return self in {\n        ItemType.THEOREM, ItemType.LEMMA, ItemType.PROPOSITION,\n        ItemType.COROLLARY, ItemType.EXAMPLE # Examples might need verification/proof\n    }\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.ItemStatus","title":"<code>ItemStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates the possible states of an item in the Knowledge Base lifecycle.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>class ItemStatus(enum.Enum):\n    \"\"\"Enumerates the possible states of an item in the Knowledge Base lifecycle.\"\"\"\n    # Initial/General States\n    PENDING = \"Pending\" # Default initial state, ready for any processing\n    ERROR = \"Error\"     # General error state\n\n    # LaTeX Processing States\n    PENDING_LATEX = \"PendingLatex\"                 # Ready for LaTeX generation\n    LATEX_GENERATION_IN_PROGRESS = \"LatexGenInProgress\"  # LLM is generating LaTeX\n    PENDING_LATEX_REVIEW = \"PendingLatexReview\"      # LaTeX generated, awaiting review\n    LATEX_REVIEW_IN_PROGRESS = \"LatexReviewInProgress\" # LLM is reviewing LaTeX\n    LATEX_ACCEPTED = \"LatexAccepted\"             # LaTeX generated and reviewer accepted\n    LATEX_REJECTED_FINAL = \"LatexRejectedFinal\"    # LaTeX rejected after max review cycles\n\n    # Lean Processing States\n    PENDING_LEAN = \"PendingLean\"                   # Ready for Lean code generation (assumes LATEX_ACCEPTED?)\n    LEAN_GENERATION_IN_PROGRESS = \"LeanGenInProgress\"   # LLM is generating Lean code\n    LEAN_VALIDATION_PENDING = \"LeanValidationPending\" # Lean generated, awaiting check_and_compile\n    LEAN_VALIDATION_IN_PROGRESS = \"LeanValidationInProgress\" # check_and_compile is running\n    LEAN_VALIDATION_FAILED = \"LeanValidationFailed\"   # check_and_compile failed\n    PROVEN = \"Proven\"                           # Lean code successfully validated (Implies LATEX_ACCEPTED)\n\n    # Specific Proven States (Map to PROVEN for simplicity in logic, but retain for potential filtering)\n    AXIOM_ACCEPTED = \"Proven\" # Axioms are accepted, not strictly proven\n    DEFINITION_ADDED = \"Proven\" # Definitions are added\n    REMARK_ADDED = \"Proven\"\n    NOTATION_ADDED = \"Proven\"\n    STRUCTURE_ADDED = \"Proven\"\n    # EXAMPLE_VERIFIED = \"Proven\" # Maybe example needs proof, keep PROVEN as main state\n\n    # Other States\n    CONJECTURE_STATED = \"Pending\" # Conjectures start as Pending, awaiting proof attempt\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.LatexLink","title":"<code>LatexLink</code>  <code>dataclass</code>","text":"<p>Represents a link to a specific component in an external LaTeX source.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>@dataclass\nclass LatexLink:\n    \"\"\"Represents a link to a specific component in an external LaTeX source.\"\"\"\n    citation_text: str\n    link_type: str = 'statement'\n    source_identifier: Optional[str] = None\n    latex_snippet: Optional[str] = None\n    match_confidence: Optional[float] = None\n    verified_by_human: bool = False\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; 'LatexLink':\n        if 'link_type' not in data: data['link_type'] = 'statement'\n        try: return cls(**data)\n        except TypeError as e: raise TypeError(f\"Error creating LatexLink from dict: {e}\") from e\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem","title":"<code>KBItem</code>  <code>dataclass</code>","text":"<p>Represents a single node (goal) in the mathematical Knowledge Base.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>@dataclass\nclass KBItem:\n    \"\"\"Represents a single node (goal) in the mathematical Knowledge Base.\"\"\"\n    # Core Identification\n    id: Optional[int] = None\n    unique_name: str = field(default_factory=lambda: f\"item_{uuid.uuid4().hex}\")\n    item_type: ItemType = ItemType.THEOREM\n\n    # Content\n    description_nl: str = \"\"\n    latex_statement: Optional[str] = None # Contains the corresponding latex statement\n    latex_proof: Optional[str] = None     # Added field for informal proof\n    lean_code: str = \"\"                   # Formal Lean code (often with 'sorry')\n\n    # Embeddings (as raw bytes)\n    embedding_nl: Optional[bytes] = None      # Generated from description_nl\n    embedding_latex: Optional[bytes] = None # Generated ONLY from latex_statement\n\n    # Context &amp; Relationships\n    topic: str = \"General\"\n    plan_dependencies: List[str] = field(default_factory=list) # Names of items needed for the proof plan\n    dependencies: List[str] = field(default_factory=list) # Names of items discovered during Lean compilation\n    latex_links: List[LatexLink] = field(default_factory=list)\n\n    # State &amp; Metadata\n    status: ItemStatus = ItemStatus.PENDING\n    failure_count: int = 0 # General failure counter\n    latex_review_feedback: Optional[str] = None # Store reviewer feedback if LaTeX (statement or proof) rejected\n    generation_prompt: Optional[str] = None # Store last prompt sent to LLM (for any generation task)\n    raw_ai_response: Optional[str] = None # Store last raw response from LLM\n    lean_error_log: Optional[str] = None # Store output from check_and_compile_item on failure\n    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n    last_modified_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n\n    def __post_init__(self):\n        \"\"\"Basic validation after initialization.\"\"\"\n        if not self.unique_name: raise ValueError(\"unique_name cannot be empty.\")\n        if not isinstance(self.lean_code, str): self.lean_code = str(self.lean_code)\n        if self.created_at.tzinfo is None: self.created_at = self.created_at.replace(tzinfo=timezone.utc)\n        if self.last_modified_at.tzinfo is None: self.last_modified_at = self.last_modified_at.replace(tzinfo=timezone.utc)\n        # Ensure latex_proof is None if item type doesn't require proof\n        if not self.item_type.requires_proof():\n            self.latex_proof = None\n\n    def update_status(self, new_status: ItemStatus, error_log: Optional[str] = _sentinel, review_feedback: Optional[str] = _sentinel):\n        \"\"\"Updates status, error log (optional), review feedback (optional), and modification time.\"\"\"\n        if not isinstance(new_status, ItemStatus):\n             raise TypeError(f\"Invalid status type: {type(new_status)}. Must be ItemStatus enum.\")\n        self.status = new_status\n        if error_log is not _sentinel:\n             self.lean_error_log = error_log\n        if review_feedback is not _sentinel:\n             self.latex_review_feedback = review_feedback\n\n        # Clear logs if moving to a relevant \"good\" state\n        if new_status == ItemStatus.PROVEN:\n            self.lean_error_log = None\n            self.latex_review_feedback = None # Assume LaTeX was accepted earlier\n        if new_status == ItemStatus.LATEX_ACCEPTED:\n            self.latex_review_feedback = None\n\n        self.last_modified_at = datetime.now(timezone.utc)\n\n    def add_plan_dependency(self, dep_unique_name: str):\n        \"\"\"Adds a planning dependency if not already present.\"\"\"\n        if dep_unique_name not in self.plan_dependencies:\n            self.plan_dependencies.append(dep_unique_name)\n            self.last_modified_at = datetime.now(timezone.utc)\n\n    def add_dependency(self, dep_unique_name: str):\n        \"\"\"Adds a discovered Lean dependency if not already present.\"\"\"\n        if dep_unique_name not in self.dependencies:\n            self.dependencies.append(dep_unique_name)\n            self.last_modified_at = datetime.now(timezone.utc)\n\n    def add_latex_link(self, link: LatexLink):\n        \"\"\"Adds a LaTeX link.\"\"\"\n        self.latex_links.append(link)\n        self.last_modified_at = datetime.now(timezone.utc)\n\n    def increment_failure_count(self):\n        \"\"\"Increments the general failure count.\"\"\"\n        self.failure_count += 1\n        self.last_modified_at = datetime.now(timezone.utc)\n\n    # update_olean method removed\n\n    # --- Serialization ---\n    def to_dict_for_db(self) -&gt; Dict[str, Any]:\n        \"\"\"Serializes to dict for DB, converting complex types.\"\"\"\n        # Ensure proof is None if not required\n        proof = self.latex_proof if self.item_type.requires_proof() else None\n        # Exclude embeddings here, they are handled separately in save_kb_item\n        return {\n            \"id\": self.id,\n            \"unique_name\": self.unique_name,\n            \"item_type\": self.item_type.name,\n            \"description_nl\": self.description_nl,\n            \"latex_statement\": self.latex_statement,\n            \"latex_proof\": proof,\n            \"lean_code\": self.lean_code,\n            \"topic\": self.topic,\n            \"plan_dependencies\": json.dumps(self.plan_dependencies),\n            \"dependencies\": json.dumps(self.dependencies),\n            \"latex_links\": json.dumps([asdict(link) for link in self.latex_links]),\n            \"status\": self.status.name,\n            \"failure_count\": self.failure_count,\n            \"latex_review_feedback\": self.latex_review_feedback,\n            \"generation_prompt\": self.generation_prompt,\n            \"raw_ai_response\": self.raw_ai_response,\n            \"lean_error_log\": self.lean_error_log,\n            \"created_at\": self.created_at.isoformat(),\n            \"last_modified_at\": self.last_modified_at.isoformat()\n        }\n\n    @classmethod\n    def from_db_dict(cls, data: Dict[str, Any]) -&gt; 'KBItem':\n        \"\"\"Creates KBItem from DB dictionary row.\"\"\"\n        try:\n            item_type_str = data[\"item_type\"]\n            item_type = ItemType[item_type_str]\n\n            status_str = data.get(\"status\", ItemStatus.PENDING.name)\n            try:\n                status = ItemStatus[status_str]\n            except KeyError:\n                warnings.warn(f\"Invalid status '{status_str}' for item {data.get('unique_name')}. Defaulting to PENDING.\")\n                status = ItemStatus.PENDING\n\n            failure_count = data.get(\"failure_count\", 0)\n            if not isinstance(failure_count, int):\n                warnings.warn(f\"Invalid type for failure_count: got {type(failure_count)}. Using 0.\")\n                failure_count = 0\n\n            # Only load latex_proof if the item type requires it\n            latex_proof = data.get(\"latex_proof\") if item_type.requires_proof() else None\n\n            return cls(\n                id=data.get(\"id\"),\n                unique_name=data[\"unique_name\"],\n                item_type=item_type,\n                description_nl=data.get(\"description_nl\", \"\"),\n                latex_statement=data.get(\"latex_statement\"),\n                latex_proof=latex_proof,\n                lean_code=data.get(\"lean_code\", \"\"),\n                embedding_nl=data.get(\"embedding_nl\"),\n                embedding_latex=data.get(\"embedding_latex\"),\n                topic=data.get(\"topic\", \"General\"),\n                plan_dependencies=json.loads(data.get(\"plan_dependencies\") or '[]'),\n                dependencies=json.loads(data.get(\"dependencies\") or '[]'),\n                latex_links=[LatexLink.from_dict(link_data) for link_data in json.loads(data.get(\"latex_links\") or '[]')],\n                status=status,\n                failure_count=failure_count,\n                latex_review_feedback=data.get(\"latex_review_feedback\"),\n                generation_prompt=data.get(\"generation_prompt\"),\n                raw_ai_response=data.get(\"raw_ai_response\"),\n                lean_error_log=data.get(\"lean_error_log\"),\n                created_at=datetime.fromisoformat(data[\"created_at\"]),\n                last_modified_at=datetime.fromisoformat(data[\"last_modified_at\"])\n             )\n        except (KeyError, json.JSONDecodeError, ValueError, TypeError) as e:\n             raise ValueError(f\"Error deserializing KBItem '{data.get('unique_name', 'UNKNOWN_ITEM')}' from DB dict: {e}\") from e\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem-functions","title":"Functions","text":""},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Basic validation after initialization.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Basic validation after initialization.\"\"\"\n    if not self.unique_name: raise ValueError(\"unique_name cannot be empty.\")\n    if not isinstance(self.lean_code, str): self.lean_code = str(self.lean_code)\n    if self.created_at.tzinfo is None: self.created_at = self.created_at.replace(tzinfo=timezone.utc)\n    if self.last_modified_at.tzinfo is None: self.last_modified_at = self.last_modified_at.replace(tzinfo=timezone.utc)\n    # Ensure latex_proof is None if item type doesn't require proof\n    if not self.item_type.requires_proof():\n        self.latex_proof = None\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem.update_status","title":"<code>update_status(new_status: ItemStatus, error_log: Optional[str] = _sentinel, review_feedback: Optional[str] = _sentinel)</code>","text":"<p>Updates status, error log (optional), review feedback (optional), and modification time.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def update_status(self, new_status: ItemStatus, error_log: Optional[str] = _sentinel, review_feedback: Optional[str] = _sentinel):\n    \"\"\"Updates status, error log (optional), review feedback (optional), and modification time.\"\"\"\n    if not isinstance(new_status, ItemStatus):\n         raise TypeError(f\"Invalid status type: {type(new_status)}. Must be ItemStatus enum.\")\n    self.status = new_status\n    if error_log is not _sentinel:\n         self.lean_error_log = error_log\n    if review_feedback is not _sentinel:\n         self.latex_review_feedback = review_feedback\n\n    # Clear logs if moving to a relevant \"good\" state\n    if new_status == ItemStatus.PROVEN:\n        self.lean_error_log = None\n        self.latex_review_feedback = None # Assume LaTeX was accepted earlier\n    if new_status == ItemStatus.LATEX_ACCEPTED:\n        self.latex_review_feedback = None\n\n    self.last_modified_at = datetime.now(timezone.utc)\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem.add_plan_dependency","title":"<code>add_plan_dependency(dep_unique_name: str)</code>","text":"<p>Adds a planning dependency if not already present.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def add_plan_dependency(self, dep_unique_name: str):\n    \"\"\"Adds a planning dependency if not already present.\"\"\"\n    if dep_unique_name not in self.plan_dependencies:\n        self.plan_dependencies.append(dep_unique_name)\n        self.last_modified_at = datetime.now(timezone.utc)\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem.add_dependency","title":"<code>add_dependency(dep_unique_name: str)</code>","text":"<p>Adds a discovered Lean dependency if not already present.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def add_dependency(self, dep_unique_name: str):\n    \"\"\"Adds a discovered Lean dependency if not already present.\"\"\"\n    if dep_unique_name not in self.dependencies:\n        self.dependencies.append(dep_unique_name)\n        self.last_modified_at = datetime.now(timezone.utc)\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem.add_latex_link","title":"<code>add_latex_link(link: LatexLink)</code>","text":"<p>Adds a LaTeX link.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def add_latex_link(self, link: LatexLink):\n    \"\"\"Adds a LaTeX link.\"\"\"\n    self.latex_links.append(link)\n    self.last_modified_at = datetime.now(timezone.utc)\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem.increment_failure_count","title":"<code>increment_failure_count()</code>","text":"<p>Increments the general failure count.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def increment_failure_count(self):\n    \"\"\"Increments the general failure count.\"\"\"\n    self.failure_count += 1\n    self.last_modified_at = datetime.now(timezone.utc)\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem.to_dict_for_db","title":"<code>to_dict_for_db() -&gt; Dict[str, Any]</code>","text":"<p>Serializes to dict for DB, converting complex types.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def to_dict_for_db(self) -&gt; Dict[str, Any]:\n    \"\"\"Serializes to dict for DB, converting complex types.\"\"\"\n    # Ensure proof is None if not required\n    proof = self.latex_proof if self.item_type.requires_proof() else None\n    # Exclude embeddings here, they are handled separately in save_kb_item\n    return {\n        \"id\": self.id,\n        \"unique_name\": self.unique_name,\n        \"item_type\": self.item_type.name,\n        \"description_nl\": self.description_nl,\n        \"latex_statement\": self.latex_statement,\n        \"latex_proof\": proof,\n        \"lean_code\": self.lean_code,\n        \"topic\": self.topic,\n        \"plan_dependencies\": json.dumps(self.plan_dependencies),\n        \"dependencies\": json.dumps(self.dependencies),\n        \"latex_links\": json.dumps([asdict(link) for link in self.latex_links]),\n        \"status\": self.status.name,\n        \"failure_count\": self.failure_count,\n        \"latex_review_feedback\": self.latex_review_feedback,\n        \"generation_prompt\": self.generation_prompt,\n        \"raw_ai_response\": self.raw_ai_response,\n        \"lean_error_log\": self.lean_error_log,\n        \"created_at\": self.created_at.isoformat(),\n        \"last_modified_at\": self.last_modified_at.isoformat()\n    }\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.KBItem.from_db_dict","title":"<code>from_db_dict(data: Dict[str, Any]) -&gt; KBItem</code>  <code>classmethod</code>","text":"<p>Creates KBItem from DB dictionary row.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>@classmethod\ndef from_db_dict(cls, data: Dict[str, Any]) -&gt; 'KBItem':\n    \"\"\"Creates KBItem from DB dictionary row.\"\"\"\n    try:\n        item_type_str = data[\"item_type\"]\n        item_type = ItemType[item_type_str]\n\n        status_str = data.get(\"status\", ItemStatus.PENDING.name)\n        try:\n            status = ItemStatus[status_str]\n        except KeyError:\n            warnings.warn(f\"Invalid status '{status_str}' for item {data.get('unique_name')}. Defaulting to PENDING.\")\n            status = ItemStatus.PENDING\n\n        failure_count = data.get(\"failure_count\", 0)\n        if not isinstance(failure_count, int):\n            warnings.warn(f\"Invalid type for failure_count: got {type(failure_count)}. Using 0.\")\n            failure_count = 0\n\n        # Only load latex_proof if the item type requires it\n        latex_proof = data.get(\"latex_proof\") if item_type.requires_proof() else None\n\n        return cls(\n            id=data.get(\"id\"),\n            unique_name=data[\"unique_name\"],\n            item_type=item_type,\n            description_nl=data.get(\"description_nl\", \"\"),\n            latex_statement=data.get(\"latex_statement\"),\n            latex_proof=latex_proof,\n            lean_code=data.get(\"lean_code\", \"\"),\n            embedding_nl=data.get(\"embedding_nl\"),\n            embedding_latex=data.get(\"embedding_latex\"),\n            topic=data.get(\"topic\", \"General\"),\n            plan_dependencies=json.loads(data.get(\"plan_dependencies\") or '[]'),\n            dependencies=json.loads(data.get(\"dependencies\") or '[]'),\n            latex_links=[LatexLink.from_dict(link_data) for link_data in json.loads(data.get(\"latex_links\") or '[]')],\n            status=status,\n            failure_count=failure_count,\n            latex_review_feedback=data.get(\"latex_review_feedback\"),\n            generation_prompt=data.get(\"generation_prompt\"),\n            raw_ai_response=data.get(\"raw_ai_response\"),\n            lean_error_log=data.get(\"lean_error_log\"),\n            created_at=datetime.fromisoformat(data[\"created_at\"]),\n            last_modified_at=datetime.fromisoformat(data[\"last_modified_at\"])\n         )\n    except (KeyError, json.JSONDecodeError, ValueError, TypeError) as e:\n         raise ValueError(f\"Error deserializing KBItem '{data.get('unique_name', 'UNKNOWN_ITEM')}' from DB dict: {e}\") from e\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage-functions","title":"Functions","text":""},{"location":"reference/kb_storage/#lean_automator.kb_storage.get_db_connection","title":"<code>get_db_connection(db_path: Optional[str] = None) -&gt; sqlite3.Connection</code>","text":"<p>Establishes a connection using specified path or default.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def get_db_connection(db_path: Optional[str] = None) -&gt; sqlite3.Connection:\n    \"\"\"Establishes a connection using specified path or default.\"\"\"\n    effective_path = db_path if db_path is not None else DEFAULT_DB_PATH\n    conn = sqlite3.connect(effective_path, detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES)\n    conn.row_factory = sqlite3.Row\n    conn.execute(\"PRAGMA foreign_keys = ON;\")\n    return conn\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.initialize_database","title":"<code>initialize_database(db_path: Optional[str] = None)</code>","text":"<p>Creates/updates the kb_items table schema and indexes. Assumes starting fresh or columns already exist.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def initialize_database(db_path: Optional[str] = None):\n    \"\"\"Creates/updates the kb_items table schema and indexes. Assumes starting fresh or columns already exist.\"\"\"\n    effective_path = db_path if db_path is not None else DEFAULT_DB_PATH\n    logger.info(f\"Initializing database schema in {effective_path}...\")\n    with get_db_connection(db_path) as conn:\n        cursor = conn.cursor()\n        # Base table definition with correct, final column names\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS kb_items (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                unique_name TEXT UNIQUE NOT NULL,\n                item_type TEXT NOT NULL,\n                description_nl TEXT,\n                latex_statement TEXT,\n                latex_proof TEXT,\n                lean_code TEXT NOT NULL,\n                embedding_nl BLOB,\n                embedding_latex BLOB,\n                topic TEXT,\n                plan_dependencies TEXT,\n                dependencies TEXT,\n                latex_links TEXT,\n                status TEXT NOT NULL,\n                failure_count INTEGER DEFAULT 0 NOT NULL,\n                latex_review_feedback TEXT,\n                generation_prompt TEXT,\n                raw_ai_response TEXT,\n                lean_error_log TEXT,\n                created_at TEXT NOT NULL,\n                last_modified_at TEXT NOT NULL\n            );\n        \"\"\")\n        # Use _add_column_if_not_exists for robustness against schema drift or future additions\n        # It will do nothing if the column already exists from CREATE TABLE.\n        _add_column_if_not_exists(cursor, \"kb_items\", \"latex_statement\", \"TEXT\")\n        _add_column_if_not_exists(cursor, \"kb_items\", \"latex_proof\", \"TEXT\")\n        _add_column_if_not_exists(cursor, \"kb_items\", \"plan_dependencies\", \"TEXT\")\n        _add_column_if_not_exists(cursor, \"kb_items\", \"failure_count\", \"INTEGER\", default_value=0)\n        _add_column_if_not_exists(cursor, \"kb_items\", \"embedding_latex\", \"BLOB\")\n        _add_column_if_not_exists(cursor, \"kb_items\", \"embedding_nl\", \"BLOB\")\n        _add_column_if_not_exists(cursor, \"kb_items\", \"latex_review_feedback\", \"TEXT\")\n\n        # Create indexes\n        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_kbitem_unique_name ON kb_items (unique_name);\")\n        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_kbitem_type ON kb_items (item_type);\")\n        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_kbitem_status ON kb_items (status);\")\n        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_kbitem_topic ON kb_items (topic);\")\n        conn.commit()\n    logger.info(\"Database schema initialization complete.\")\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.save_kb_item","title":"<code>save_kb_item(item: KBItem, client: Optional[GeminiClient] = None, db_path: Optional[str] = None) -&gt; KBItem</code>  <code>async</code>","text":"<p>Saves (Inserts or Updates) a KBItem, including handling embedding generation based ONLY on latex_statement or description_nl if they have changed and a client is provided.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>KBItem</code> <p>The KBItem object to save.</p> required <code>client</code> <code>Optional[GeminiClient]</code> <p>An initialized GeminiClient instance (required for embedding generation).</p> <code>None</code> <code>db_path</code> <code>Optional[str]</code> <p>Optional path to the database file.</p> <code>None</code> <p>Returns:</p> Type Description <code>KBItem</code> <p>The saved KBItem object (potentially updated with id and embeddings).</p> <p>Raises:</p> Type Description <code>Error</code> <p>If a database error occurs.</p> <code>TypeError</code> <p>If item enums are invalid.</p> <code>ValueError</code> <p>If embedding generation fails during an attempt.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>async def save_kb_item(item: KBItem, client: Optional[GeminiClient] = None, db_path: Optional[str] = None) -&gt; KBItem:\n    \"\"\"\n    Saves (Inserts or Updates) a KBItem, including handling embedding generation\n    based ONLY on latex_statement or description_nl if they have changed\n    and a client is provided.\n\n    Args:\n        item: The KBItem object to save.\n        client: An initialized GeminiClient instance (required for embedding generation).\n        db_path: Optional path to the database file.\n\n    Returns:\n        The saved KBItem object (potentially updated with id and embeddings).\n\n    Raises:\n        sqlite3.Error: If a database error occurs.\n        TypeError: If item enums are invalid.\n        ValueError: If embedding generation fails during an attempt.\n    \"\"\"\n    if not isinstance(item.item_type, ItemType): raise TypeError(f\"item.item_type invalid: {item.item_type}\")\n    if not isinstance(item.status, ItemStatus): raise TypeError(f\"item.status invalid: {item.status}\")\n\n    # Ensure latex_proof is None if item type doesn't require it\n    if not item.item_type.requires_proof():\n        item.latex_proof = None\n\n    # --- Get existing item state from DB (if it exists) ---\n    existing_item: Optional[KBItem] = None\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    if item.id is not None:\n        existing_item = get_kb_item_by_id(item.id, effective_db_path)\n    if existing_item is None and item.unique_name:\n         existing_item = get_kb_item_by_name(item.unique_name, effective_db_path)\n\n    # --- Determine if text fields relevant for embeddings changed ---\n    latex_statement_changed = False # Embeddings based on STATEMENT only\n    nl_changed = False\n    if existing_item:\n        if item.latex_statement != existing_item.latex_statement:\n            latex_statement_changed = True\n        if item.description_nl != existing_item.description_nl:\n            nl_changed = True\n    else: # For new items, consider text changed if it's present\n        if item.latex_statement: latex_statement_changed = True\n        if item.description_nl: nl_changed = True\n\n    # --- Prepare main data for UPSERT (excluding embeddings) ---\n    item.last_modified_at = datetime.now(timezone.utc) # Ensure modification time is updated\n    db_data = item.to_dict_for_db()\n    db_data_id = db_data.pop('id', None)\n    db_data.pop('embedding_latex', None) # Handled separately below\n    db_data.pop('embedding_nl', None)    # Handled separately below\n\n    columns = ', '.join(db_data.keys())\n    placeholders = ', '.join('?' * len(db_data))\n    update_setters = ', '.join(f\"{key} = excluded.{key}\" for key in db_data.keys() if key != 'unique_name')\n\n    sql_upsert = f\"\"\"\n        INSERT INTO kb_items ({columns})\n        VALUES ({placeholders})\n        ON CONFLICT(unique_name) DO UPDATE SET\n        {update_setters}\n        RETURNING id;\n    \"\"\"\n    params_upsert = tuple(db_data.values())\n\n    # --- Embedding Logic (based on latex_statement, not latex_proof) ---\n    final_latex_bytes = item.embedding_latex\n    final_nl_bytes = item.embedding_nl\n    should_generate_latex = latex_statement_changed and item.latex_statement and client\n    should_generate_nl = nl_changed and item.description_nl and client\n\n    kb_search = None\n    if should_generate_latex or should_generate_nl:\n        try:\n            from lean_automator import kb_search as kb_search_module\n            kb_search = kb_search_module\n        except ImportError:\n            warnings.warn(\"kb_search module not found when needed for embedding generation.\", ImportWarning)\n            kb_search = None\n\n    should_generate_latex = should_generate_latex and kb_search\n    should_generate_nl = should_generate_nl and kb_search\n\n    if (latex_statement_changed and item.latex_statement and not should_generate_latex):\n         warnings.warn(f\"Cannot generate LaTeX statement embedding for '{item.unique_name}'. Client/kb_search missing or statement empty.\")\n    if (nl_changed and item.description_nl and not should_generate_nl):\n         warnings.warn(f\"Cannot generate NL embedding for '{item.unique_name}'. Client/kb_search missing or text empty.\")\n\n    if should_generate_latex or should_generate_nl:\n        tasks = []\n        tasks.append(\n             kb_search.generate_embedding(item.latex_statement, EMBEDDING_TASK_TYPE_DOCUMENT, client) # Use latex_statement\n             if should_generate_latex else asyncio.sleep(0, result=None)\n        )\n        tasks.append(\n             kb_search.generate_embedding(item.description_nl, EMBEDDING_TASK_TYPE_DOCUMENT, client)\n             if should_generate_nl else asyncio.sleep(0, result=None)\n        )\n        try:\n            embedding_results = await asyncio.gather(*tasks)\n            generated_latex_np = embedding_results[0]\n            generated_nl_np = embedding_results[1]\n\n            if generated_latex_np is not None:\n                final_latex_bytes = generated_latex_np.astype(EMBEDDING_DTYPE).tobytes()\n                logger.debug(f\"Generated LaTeX statement embedding for {item.unique_name}\")\n            elif should_generate_latex:\n                 warnings.warn(f\"Failed to generate LaTeX statement embedding for '{item.unique_name}'. Using previous value if any.\")\n\n            if generated_nl_np is not None:\n                final_nl_bytes = generated_nl_np.astype(EMBEDDING_DTYPE).tobytes()\n                logger.debug(f\"Generated NL embedding for {item.unique_name}\")\n            elif should_generate_nl:\n                 warnings.warn(f\"Failed to generate NL embedding for '{item.unique_name}'. Using previous value if any.\")\n        except Exception as e:\n             warnings.warn(f\"Error during embedding generation task for '{item.unique_name}': {e}. Using previous values if any.\")\n\n    # --- Determine if embedding columns need DB update ---\n    existing_latex_bytes = existing_item.embedding_latex if existing_item else None\n    existing_nl_bytes = existing_item.embedding_nl if existing_item else None\n    update_latex_in_db = (final_latex_bytes != existing_latex_bytes)\n    update_nl_in_db = (final_nl_bytes != existing_nl_bytes)\n\n    # Update item object state AFTER potential generation attempt\n    item.embedding_latex = final_latex_bytes\n    item.embedding_nl = final_nl_bytes\n\n    # --- Database Operations ---\n    retrieved_id = None\n    with get_db_connection(effective_db_path) as conn:\n        cursor = conn.cursor()\n        try:\n            # 1. Upsert main item data using RETURNING id\n            cursor.execute(sql_upsert, params_upsert)\n            result = cursor.fetchone()\n            if result and result['id'] is not None:\n                retrieved_id = result['id']\n                item.id = retrieved_id # Update item object with confirmed ID\n            else:\n                # Fallback\n                cursor.execute(\"SELECT id FROM kb_items WHERE unique_name = ?\", (item.unique_name,))\n                result = cursor.fetchone()\n                if result: item.id = result['id']\n                else: raise sqlite3.OperationalError(f\"Failed to retrieve ID after saving {item.unique_name}\")\n            logger.debug(f\"Upserted item {item.unique_name}, got ID: {item.id}\")\n\n            # 2. Conditionally update embeddings using separate UPDATE statements\n            if item.id is None:\n                 raise sqlite3.OperationalError(f\"Cannot update embeddings, ID is missing for {item.unique_name}\")\n\n            if update_latex_in_db:\n                 logger.debug(f\"Updating embedding_latex for item ID {item.id}\")\n                 cursor.execute(\"UPDATE kb_items SET embedding_latex = ? WHERE id = ?\", (final_latex_bytes, item.id))\n            if update_nl_in_db:\n                 logger.debug(f\"Updating embedding_nl for item ID {item.id}\")\n                 cursor.execute(\"UPDATE kb_items SET embedding_nl = ? WHERE id = ?\", (final_nl_bytes, item.id))\n\n            conn.commit()\n            logger.info(f\"Successfully saved KBItem '{item.unique_name}' (ID: {item.id})\")\n        except sqlite3.Error as e:\n            logger.error(f\"Database error during save_kb_item for {item.unique_name}: {e}\")\n            conn.rollback()\n            raise\n\n    return item # Return the item (updated with ID and potentially new embeddings)\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.get_kb_item_by_id","title":"<code>get_kb_item_by_id(item_id: int, db_path: Optional[str] = None) -&gt; Optional[KBItem]</code>","text":"<p>Retrieves a KBItem by its primary key ID.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def get_kb_item_by_id(item_id: int, db_path: Optional[str] = None) -&gt; Optional[KBItem]:\n    \"\"\"Retrieves a KBItem by its primary key ID.\"\"\"\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    with get_db_connection(effective_db_path) as conn:\n        cursor = conn.cursor()\n        # Best practice is to list columns explicitly if schema might vary\n        cursor.execute(\"SELECT * FROM kb_items WHERE id = ?\", (item_id,))\n        row = cursor.fetchone()\n        if row:\n            try:\n                 return KBItem.from_db_dict(dict(row))\n            except ValueError as e:\n                logger.error(f\"Error deserializing KBItem id={item_id}: {e}\")\n                return None\n    return None\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.get_kb_item_by_name","title":"<code>get_kb_item_by_name(unique_name: str, db_path: Optional[str] = None) -&gt; Optional[KBItem]</code>","text":"<p>Retrieves a KBItem by its unique name.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def get_kb_item_by_name(unique_name: str, db_path: Optional[str] = None) -&gt; Optional[KBItem]:\n    \"\"\"Retrieves a KBItem by its unique name.\"\"\"\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    with get_db_connection(effective_db_path) as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM kb_items WHERE unique_name = ?\", (unique_name,))\n        row = cursor.fetchone()\n        if row:\n             try:\n                 return KBItem.from_db_dict(dict(row))\n             except ValueError as e:\n                 logger.error(f\"Error deserializing KBItem name='{unique_name}': {e}\")\n                 return None\n    return None\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.get_items_by_status","title":"<code>get_items_by_status(status: ItemStatus, db_path: Optional[str] = None) -&gt; Generator[KBItem, None, None]</code>","text":"<p>Yields KBItems matching a specific status.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def get_items_by_status(status: ItemStatus, db_path: Optional[str] = None) -&gt; Generator[KBItem, None, None]:\n    \"\"\"Yields KBItems matching a specific status.\"\"\"\n    if not isinstance(status, ItemStatus): raise TypeError(\"status must be an ItemStatus enum member\")\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    with get_db_connection(effective_db_path) as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM kb_items WHERE status = ?\", (status.name,))\n        for row in cursor:\n            try:\n                yield KBItem.from_db_dict(dict(row))\n            except ValueError as e:\n                logger.error(f\"Error deserializing KBItem '{row.get('unique_name', 'UNKNOWN')}' fetching status '{status.name}': {e}\")\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.get_items_by_topic","title":"<code>get_items_by_topic(topic_prefix: str, db_path: Optional[str] = None) -&gt; Generator[KBItem, None, None]</code>","text":"<p>Yields KBItems whose topic starts with the given prefix.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def get_items_by_topic(topic_prefix: str, db_path: Optional[str] = None) -&gt; Generator[KBItem, None, None]:\n    \"\"\"Yields KBItems whose topic starts with the given prefix.\"\"\"\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    with get_db_connection(effective_db_path) as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM kb_items WHERE topic LIKE ?\", (f\"{topic_prefix}%\",))\n        for row in cursor:\n            try:\n                yield KBItem.from_db_dict(dict(row))\n            except ValueError as e:\n                logger.error(f\"Error deserializing KBItem '{row.get('unique_name', 'UNKNOWN')}' fetching topic '{topic_prefix}': {e}\")\n</code></pre>"},{"location":"reference/kb_storage/#lean_automator.kb_storage.get_all_items_with_embedding","title":"<code>get_all_items_with_embedding(embedding_field: str, db_path: Optional[str] = None) -&gt; List[Tuple[int, str, bytes]]</code>","text":"<p>Retrieves ID, name, and embedding blob for all items having a non-NULL value in the specified embedding field.</p> Source code in <code>lean_automator/kb_storage.py</code> <pre><code>def get_all_items_with_embedding(embedding_field: str, db_path: Optional[str] = None) -&gt; List[Tuple[int, str, bytes]]:\n    \"\"\"Retrieves ID, name, and embedding blob for all items having a non-NULL value in the specified embedding field.\"\"\"\n    if embedding_field not in ['embedding_nl', 'embedding_latex']:\n        raise ValueError(\"embedding_field must be 'embedding_nl' or 'embedding_latex'\")\n    items_with_embeddings = []\n    sql = f\"SELECT id, unique_name, {embedding_field} FROM kb_items WHERE {embedding_field} IS NOT NULL;\"\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    with get_db_connection(effective_db_path) as conn:\n        cursor = conn.cursor()\n        try:\n            cursor.execute(sql)\n            for row in cursor:\n                blob = row[embedding_field]\n                if isinstance(blob, bytes):\n                     items_with_embeddings.append((row['id'], row['unique_name'], blob))\n                else:\n                     logger.warning(f\"Expected bytes for embedding field {embedding_field} on item ID {row['id']}, but got {type(blob)}. Skipping.\")\n        except sqlite3.Error as e:\n            logger.error(f\"Database error retrieving embeddings for field {embedding_field}: {e}\")\n    return items_with_embeddings\n</code></pre>"},{"location":"reference/latex_processor/","title":"LaTeX Processor","text":"<p>Handles LaTeX generation and review for Knowledge Base items using an LLM.</p> <p>This module orchestrates the process of generating LaTeX representations (both statement and optional informal proof) for mathematical items stored in the Knowledge Base (<code>KBItem</code>). It uses a provided LLM client (<code>GeminiClient</code>) to generate and subsequently review the LaTeX content based on predefined prompts and the item's context (description, dependencies). The process involves iterative refinement cycles until the LaTeX is accepted by the LLM reviewer or a maximum number of cycles is reached. It updates the status and content of the KBItem in the database accordingly.</p>"},{"location":"reference/latex_processor/#lean_automator.latex_processor","title":"<code>lean_automator.latex_processor</code>","text":""},{"location":"reference/latex_processor/#lean_automator.latex_processor-classes","title":"Classes","text":""},{"location":"reference/latex_processor/#lean_automator.latex_processor-functions","title":"Functions","text":""},{"location":"reference/latex_processor/#lean_automator.latex_processor.generate_and_review_latex","title":"<code>generate_and_review_latex(unique_name: str, client: GeminiClient, db_path: Optional[str] = None) -&gt; bool</code>  <code>async</code>","text":"<p>Manages the combined process of generating, reviewing, and refining LaTeX statement and proof (if applicable) for a given KBItem. Updates the item in the database.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if LaTeX was successfully generated and accepted, False otherwise.</p> Source code in <code>lean_automator/latex_processor.py</code> <pre><code>async def generate_and_review_latex(\n    unique_name: str,\n    client: GeminiClient,\n    db_path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"\n    Manages the combined process of generating, reviewing, and refining LaTeX statement\n    and proof (if applicable) for a given KBItem. Updates the item in the database.\n\n    Returns:\n        bool: True if LaTeX was successfully generated and accepted, False otherwise.\n    \"\"\"\n    if not all([KBItem, ItemStatus, ItemType, get_kb_item_by_name, save_kb_item]):\n        logger.critical(\"KB Storage module components not loaded correctly. Cannot process LaTeX.\")\n        return False\n    if not client:\n        logger.error(\"GeminiClient not provided. Cannot process LaTeX.\")\n        return False\n\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    item = get_kb_item_by_name(unique_name, effective_db_path)\n\n    if not item:\n        logger.error(f\"LaTeX Processing: Item not found: {unique_name}\")\n        return False\n\n    # Define statuses that trigger processing\n    trigger_statuses = {ItemStatus.PENDING, ItemStatus.PENDING_LATEX, ItemStatus.LATEX_REJECTED_FINAL}\n    # Define statuses that mean LaTeX is already acceptable\n    already_accepted_statuses = {ItemStatus.LATEX_ACCEPTED, ItemStatus.PROVEN, ItemStatus.DEFINITION_ADDED, ItemStatus.AXIOM_ACCEPTED}\n\n    if item.status in already_accepted_statuses:\n        logger.info(f\"LaTeX Processing: Item {unique_name} status ({item.status.name}) indicates LaTeX is already acceptable. Skipping.\")\n        return True\n\n    if item.status not in trigger_statuses:\n        logger.warning(f\"LaTeX Processing: Item {unique_name} not in a trigger status ({ {s.name for s in trigger_statuses} }). Current: {item.status.name}. Skipping.\")\n        return False\n\n    original_status = item.status\n\n    # --- Fetch Dependencies ---\n    dependency_items: List[KBItem] = []\n    logger.debug(f\"Fetching dependencies for {unique_name}: {item.plan_dependencies}\")\n    for dep_name in item.plan_dependencies:\n        dep_item = get_kb_item_by_name(dep_name, effective_db_path)\n        if dep_item:\n            # Need dependency's statement to be accepted\n            if dep_item.latex_statement and dep_item.status in already_accepted_statuses:\n                 dependency_items.append(dep_item)\n            else:\n                 logger.warning(f\"Dependency '{dep_name}' for '{unique_name}' does not have accepted LaTeX statement (status: {dep_item.status.name}). Excluding from context.\")\n        else:\n            logger.error(f\"Dependency '{dep_name}' not found in KB for target '{unique_name}' during LaTeX processing.\")\n\n\n    # --- Start Processing ---\n    try:\n        item.update_status(ItemStatus.LATEX_GENERATION_IN_PROGRESS)\n        await save_kb_item(item, client=None, db_path=effective_db_path)\n\n        current_statement: Optional[str] = item.latex_statement # Start with existing if any\n        current_proof: Optional[str] = item.latex_proof       # Start with existing if any\n        review_feedback: Optional[str] = item.latex_review_feedback # Use previous feedback\n        accepted = False\n        proof_expected = item.item_type.requires_proof()\n\n        for cycle in range(MAX_REVIEW_CYCLES):\n            logger.info(f\"Combined LaTeX Cycle {cycle + 1}/{MAX_REVIEW_CYCLES} for {unique_name}\")\n\n            # --- Step (a): Generate / Refine Combined LaTeX ---\n            item.update_status(ItemStatus.LATEX_GENERATION_IN_PROGRESS)\n            await save_kb_item(item, client=None, db_path=effective_db_path)\n\n            raw_generator_response = await _call_latex_statement_and_proof_generator(\n                item=item,\n                dependencies=dependency_items,\n                current_statement=current_statement,\n                current_proof=current_proof,\n                review_feedback=review_feedback,\n                client=client\n            )\n\n            if not raw_generator_response:\n                logger.warning(f\"Combined LaTeX generator failed or returned no response for {unique_name} in cycle {cycle + 1}\")\n                item.update_status(ItemStatus.ERROR, f\"LaTeX generator failed in cycle {cycle + 1}\")\n                await save_kb_item(item, client=None, db_path=effective_db_path)\n                return False\n\n            # --- Step (a.2): Parse Generator Output ---\n            parsed_statement, parsed_proof = _parse_combined_latex(raw_generator_response, item.item_type)\n\n            if not parsed_statement:\n                logger.warning(f\"Failed to parse statement from generator output for {unique_name} in cycle {cycle + 1}. Raw: {raw_generator_response[:500]}\")\n                item.update_status(ItemStatus.ERROR, f\"LaTeX generator output parsing failed (statement) in cycle {cycle + 1}\")\n                item.raw_ai_response = raw_generator_response # Save raw for debugging\n                await save_kb_item(item, client=None, db_path=effective_db_path)\n                return False # Exit on parsing failure\n\n            if proof_expected and not parsed_proof:\n                logger.warning(f\"Failed to parse proof from generator output for {unique_name} (type: {item.item_type.name}) in cycle {cycle + 1}. Raw: {raw_generator_response[:500]}\")\n                # Allow continuing to review? Maybe reviewer catches it. Or fail here? Let's fail for now if proof expected but missing.\n                item.update_status(ItemStatus.ERROR, f\"LaTeX generator output parsing failed (proof) in cycle {cycle + 1}\")\n                item.raw_ai_response = raw_generator_response # Save raw for debugging\n                await save_kb_item(item, client=None, db_path=effective_db_path)\n                return False\n\n\n            current_statement = parsed_statement # Update working versions\n            current_proof = parsed_proof\n\n            # --- Step (b): Review Combined LaTeX ---\n            item.update_status(ItemStatus.LATEX_REVIEW_IN_PROGRESS)\n            # Save candidate statement/proof before review? Or just in raw response? Let's store in raw.\n            item.raw_ai_response = raw_generator_response # Store generator output\n            await save_kb_item(item, client=None, db_path=effective_db_path)\n\n            raw_reviewer_response = await _call_latex_statement_and_proof_reviewer(\n                item_type=item.item_type,\n                latex_statement=current_statement,\n                latex_proof=current_proof, # Pass None if not expected/parsed\n                unique_name=unique_name,\n                nl_description=item.description_nl,\n                dependencies=dependency_items,\n                client=client\n            )\n\n            if not raw_reviewer_response:\n                 logger.warning(f\"LaTeX reviewer failed or returned no response for {unique_name} in cycle {cycle + 1}\")\n                 item.update_status(ItemStatus.ERROR, f\"LaTeX reviewer failed in cycle {cycle + 1}\")\n                 await save_kb_item(item, client=None, db_path=effective_db_path)\n                 return False # Exit on reviewer failure\n\n            # --- Step (c): Parse and Process Review ---\n            judgment, feedback, error_loc = _parse_combined_review(raw_reviewer_response, proof_expected)\n\n            # Get fresh item state before update\n            item = get_kb_item_by_name(unique_name, effective_db_path)\n            if not item: raise Exception(\"Item vanished during review processing\")\n\n            if judgment == \"Accepted\":\n                logger.info(f\"Combined LaTeX for {unique_name} accepted by reviewer in cycle {cycle + 1}.\")\n                accepted = True\n                item.latex_statement = current_statement # Set accepted content\n                item.latex_proof = current_proof         # Set accepted content (will be None if not proof_expected)\n                item.update_status(ItemStatus.LATEX_ACCEPTED, review_feedback=None) # Clear feedback\n                # Save final state - trigger embedding generation for latex_statement\n                await save_kb_item(item, client=client, db_path=effective_db_path)\n                break # Exit loop successfully\n            else:\n                logger.warning(f\"Combined LaTeX for {unique_name} rejected by reviewer in cycle {cycle + 1}. Location: {error_loc or 'N/A'}. Feedback: {feedback[:200]}...\")\n                review_feedback = feedback # Store feedback for the next generation attempt\n                item.latex_review_feedback = review_feedback # Save feedback to item\n                item.update_status(ItemStatus.PENDING_LATEX_REVIEW) # Indicate needs another cycle\n                await save_kb_item(item, client=None, db_path=effective_db_path) # Save feedback and status\n\n        # --- After Loop ---\n        if not accepted:\n            logger.error(f\"Combined LaTeX for {unique_name} rejected after {MAX_REVIEW_CYCLES} cycles.\")\n            item = get_kb_item_by_name(unique_name, effective_db_path) # Get final state\n            if item:\n                 item.update_status(ItemStatus.LATEX_REJECTED_FINAL, review_feedback=review_feedback) # Keep last feedback\n                 await save_kb_item(item, client=None, db_path=effective_db_path)\n            return False\n        else:\n             return True # Was accepted within the loop\n\n    except Exception as e:\n        logger.exception(f\"Unhandled exception during combined LaTeX processing for {unique_name}: {e}\")\n        try:\n             item_err = get_kb_item_by_name(unique_name, effective_db_path)\n             if item_err and item_err.status not in already_accepted_statuses and item_err.status != ItemStatus.LATEX_REJECTED_FINAL:\n                 item_err.update_status(ItemStatus.ERROR, f\"Unhandled LaTeX processor exception: {e}\")\n                 await save_kb_item(item_err, client=None, db_path=effective_db_path)\n        except Exception as final_err:\n             logger.error(f\"Failed to save final error state for {unique_name} after exception: {final_err}\")\n        return False\n</code></pre>"},{"location":"reference/latex_processor/#lean_automator.latex_processor.process_pending_latex_items","title":"<code>process_pending_latex_items(client: GeminiClient, db_path: Optional[str] = None, limit: Optional[int] = None, process_statuses: Optional[List[ItemStatus]] = None)</code>  <code>async</code>","text":"<p>Finds items needing LaTeX processing and runs the generate/review cycle.</p> Source code in <code>lean_automator/latex_processor.py</code> <pre><code>async def process_pending_latex_items(\n        client: GeminiClient,\n        db_path: Optional[str] = None,\n        limit: Optional[int] = None,\n        process_statuses: Optional[List[ItemStatus]] = None\n    ):\n    \"\"\"Finds items needing LaTeX processing and runs the generate/review cycle.\"\"\"\n    if not all([ItemStatus, get_items_by_status, get_kb_item_by_name, save_kb_item]): # Add checks\n         logger.critical(\"KB Storage components not loaded correctly. Cannot batch process.\")\n         return\n\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    if process_statuses is None:\n         process_statuses = {ItemStatus.PENDING_LATEX, ItemStatus.PENDING, ItemStatus.LATEX_REJECTED_FINAL}\n\n    processed_count = 0\n    success_count = 0\n    fail_count = 0\n    items_to_process = []\n\n    logger.info(f\"Querying for items with statuses: {[s.name for s in process_statuses]}\")\n    for status in process_statuses:\n        try:\n            items_gen = get_items_by_status(status, effective_db_path)\n            count_for_status = 0\n            for item in items_gen:\n                 if limit is not None and len(items_to_process) &gt;= limit:\n                     break\n                 items_to_process.append(item)\n                 count_for_status += 1\n            logger.debug(f\"Found {count_for_status} items with status {status.name}\")\n            if limit is not None and len(items_to_process) &gt;= limit:\n                 break\n        except Exception as e:\n            logger.error(f\"Failed to retrieve items with status {status.name}: {e}\")\n\n    if not items_to_process:\n         logger.info(\"No items found requiring LaTeX processing in the specified statuses.\")\n         return\n\n    logger.info(f\"Found {len(items_to_process)} items for LaTeX processing.\")\n\n    for item in items_to_process:\n        # Check status again before processing\n        current_item_state = get_kb_item_by_name(item.unique_name, effective_db_path)\n        if not current_item_state or current_item_state.status not in process_statuses:\n            logger.info(f\"Skipping {item.unique_name} as its status changed ({current_item_state.status.name if current_item_state else 'deleted'}) or item disappeared.\")\n            continue\n\n        logger.info(f\"--- Processing LaTeX for: {item.unique_name} (ID: {item.id}, Status: {item.status.name}) ---\")\n        try:\n            # Call the main combined processing function\n            success = await generate_and_review_latex(item.unique_name, client, effective_db_path)\n            if success:\n                success_count += 1\n            else:\n                fail_count += 1\n        except Exception as e:\n             logger.error(f\"Error processing item {item.unique_name} in batch: {e}\")\n             fail_count += 1\n             try:\n                  err_item = get_kb_item_by_name(item.unique_name, effective_db_path)\n                  if err_item and err_item.status not in {ItemStatus.PROVEN, ItemStatus.LATEX_ACCEPTED}:\n                     err_item.update_status(ItemStatus.ERROR, f\"Batch processing error: {e}\")\n                     await save_kb_item(err_item, client=None, db_path=effective_db_path)\n             except Exception as save_err:\n                  logger.error(f\"Failed to save error status for {item.unique_name} during batch: {save_err}\")\n\n        processed_count += 1\n        logger.info(f\"--- Finished processing {item.unique_name} ---\")\n        # await asyncio.sleep(0.5) # Optional delay\n\n    logger.info(f\"LaTeX Batch Processing Complete. Total Processed: {processed_count}, Succeeded: {success_count}, Failed: {fail_count}\")\n</code></pre>"},{"location":"reference/lean_interaction/","title":"Lean Interaction","text":"<p>Provides functions to interact with the Lean prover via Lake.</p> <p>This module handles the validation of Lean code associated with Knowledge Base items (<code>KBItem</code>). It defines functions to: - Create temporary Lake environments for isolated verification builds. - Fetch recursive dependencies based on <code>plan_dependencies</code>. - Execute <code>lake build</code> commands asynchronously. - Update a persistent shared Lean library with successfully verified code. - Manage database status updates based on validation outcomes.</p> <p>Configuration relies on environment variables like <code>LEAN_AUTOMATOR_SHARED_LIB_PATH</code> for the location of the persistent shared library and optionally <code>LEAN_AUTOMATOR_LAKE_CACHE</code> for caching.</p>"},{"location":"reference/lean_interaction/#lean_automator.lean_interaction","title":"<code>lean_automator.lean_interaction</code>","text":""},{"location":"reference/lean_interaction/#lean_automator.lean_interaction-classes","title":"Classes","text":""},{"location":"reference/lean_interaction/#lean_automator.lean_interaction-functions","title":"Functions","text":""},{"location":"reference/lean_interaction/#lean_automator.lean_interaction.check_and_compile_item","title":"<code>check_and_compile_item(unique_name: str, db_path: Optional[str] = None, lake_executable_path: str = 'lake', timeout_seconds: int = 120) -&gt; Tuple[bool, str]</code>  <code>async</code>","text":"<p>Checks and attempts to compile/validate a KBItem using Lake, requiring a persistent shared library (<code>LEAN_AUTOMATOR_SHARED_LIB_PATH</code>).</p> <p>On successful validation in a temporary environment, it adds the item's source code to the persistent library and triggers a build there.</p> Source code in <code>lean_automator/lean_interaction.py</code> <pre><code>async def check_and_compile_item(\n    unique_name: str,\n    db_path: Optional[str] = None,\n    lake_executable_path: str = 'lake',\n    timeout_seconds: int = 120,\n    # Removed temp_lib_name as it's hardcoded in _create_temp... now\n) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Checks and attempts to compile/validate a KBItem using Lake, requiring a\n    persistent shared library (`LEAN_AUTOMATOR_SHARED_LIB_PATH`).\n\n    On successful validation in a temporary environment, it adds the item's\n    source code to the persistent library and triggers a build there.\n    \"\"\"\n    logger.info(f\"Starting Lean validation process for: {unique_name} using shared library.\")\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    loop = asyncio.get_running_loop()\n\n    # --- Check Configuration ---\n    if not SHARED_LIB_PATH or not SHARED_LIB_PATH.is_dir():\n         logger.error(\"Shared library path not configured correctly or directory does not exist.\")\n         return False, \"Shared library path configuration error.\"\n    logger.debug(f\"Using shared library: {SHARED_LIB_PATH} (Module: {SHARED_LIB_MODULE_NAME})\")\n\n\n    # 1. Fetch Target Item (Only target needed initially for source code)\n    target_item: Optional[KBItem] = None\n    try:\n        target_item = get_kb_item_by_name(unique_name, db_path=effective_db_path)\n        if target_item is None:\n            logger.error(f\"Target item '{unique_name}' not found.\")\n            return False, f\"Target item '{unique_name}' not found.\"\n        if not target_item.lean_code:\n             logger.error(f\"Target item '{unique_name}' has no lean_code to compile.\")\n             # Try to update status, but main failure is missing code\n             try:\n                 target_item.update_status(ItemStatus.ERROR, \"Missing lean_code for compilation.\")\n                 await save_kb_item(target_item, client=None, db_path=effective_db_path)\n             except Exception: pass # Ignore DB error if item already missing code\n             return False, \"Missing lean_code for compilation.\"\n\n        # We don't strictly need to fetch *all* dependencies anymore for the temp build,\n        # as Lake will resolve them via the 'require' statement.\n        # However, _generate_imports_for_target still uses item.plan_dependencies,\n        # so the target_item fetched here should have them populated correctly.\n        logger.info(f\"Fetched target item {unique_name}.\")\n\n    except FileNotFoundError as e: # Should be caught by initial check\n         logger.error(f\"Error fetching target item {unique_name}: {e}\")\n         return False, f\"Error fetching target: {e}\"\n    except Exception as e:\n        logger.exception(f\"Unexpected error fetching target item {unique_name}: {e}\")\n        # Try to update status if possible\n        if target_item and isinstance(target_item, KBItem):\n            try:\n                target_item.update_status(ItemStatus.ERROR, f\"Unexpected fetch error: {e}\")\n                await save_kb_item(target_item, client=None, db_path=effective_db_path)\n            except Exception: pass\n        return False, f\"Unexpected error fetching target: {e}\"\n\n    # 2. Create Temporary Environment for Verification\n    temp_dir_obj = tempfile.TemporaryDirectory()\n    temp_dir_base = temp_dir_obj.name\n    temp_project_path_str : Optional[str] = None\n    target_temp_module_name : Optional[str] = None\n    temp_lib_name_used = \"TempVerifyLib\" # Define here or pass if needed\n\n    try:\n        logger.info(f\"Creating temporary verification environment in: {temp_dir_base}\")\n        temp_project_path_str, target_temp_module_name = _create_temp_env_for_verification(\n             target_item, temp_dir_base, SHARED_LIB_PATH, SHARED_LIB_MODULE_NAME, temp_lib_name_used\n        )\n        logger.info(f\"Temporary Lake project created at: {temp_project_path_str}\")\n        logger.info(f\"Target module name for temporary build: {target_temp_module_name}\")\n\n    except (ValueError, OSError, TypeError) as e:\n         logger.error(f\"Failed to create temporary verification environment for {unique_name}: {e}\")\n         # Attempt to update status if target_item exists\n         if target_item:\n             try:\n                 target_item.update_status(ItemStatus.ERROR, f\"Verification env creation error: {e}\")\n                 await save_kb_item(target_item, client=None, db_path=effective_db_path)\n             except Exception as db_err: logger.error(f\"Failed to update status after env creation error: {db_err}\")\n         try: temp_dir_obj.cleanup()\n         except Exception: pass\n         return False, f\"Verification environment creation error: {e}\"\n    except Exception as e: # Catch any other unexpected error during setup\n        logger.exception(f\"Unexpected error during verification environment setup for {unique_name}: {e}\")\n        if target_item:\n            try:\n                target_item.update_status(ItemStatus.ERROR, f\"Unexpected env setup error: {e}\")\n                await save_kb_item(target_item, client=None, db_path=effective_db_path)\n            except Exception as db_err: logger.error(f\"Failed to update status after unexpected env setup error: {db_err}\")\n        try: temp_dir_obj.cleanup()\n        except Exception: pass\n        return False, f\"Unexpected setup error: {e}\"\n\n    # Check if paths were successfully created before proceeding\n    if not temp_project_path_str or not target_temp_module_name:\n        logger.error(f\"Verification environment setup failed to return valid paths for {unique_name}.\")\n        if target_item:\n            try:\n                 target_item.update_status(ItemStatus.ERROR, \"Internal error: Env setup path generation failed.\")\n                 await save_kb_item(target_item, client=None, db_path=effective_db_path)\n            except Exception: pass\n        try: temp_dir_obj.cleanup()\n        except Exception: pass\n        return False, \"Internal error: Environment setup failed.\"\n\n    # 3. Prepare and Execute Lake Build (Temporary Verification)\n    try:\n        # --- Prepare Environment for Subprocess ---\n        subprocess_env = os.environ.copy()\n        # LEAN_PATH might not be strictly necessary if dependencies are resolved via 'require',\n        # but setting it for the stdlib is still good practice.\n        # (LEAN_PATH detection logic remains the same as before)\n        # --- Set LEAN_PATH ---\n        std_lib_path = None\n        lean_executable_path_obj = None\n        try:\n            lean_executable_path_obj = pathlib.Path(lake_executable_path).resolve() # Resolve path\n            lean_exe_candidate = lean_executable_path_obj.parent / 'lean'\n            lean_exe = str(lean_exe_candidate) if lean_exe_candidate.is_file() else 'lean'\n        except Exception as path_err:\n            logger.warning(f\"Could not resolve lean executable path from lake path '{lake_executable_path}': {path_err}. Assuming 'lean' is in PATH.\")\n            lean_exe = 'lean'\n\n        if lean_exe == 'lean' and not shutil.which('lean'):\n             logger.warning(\"Could not find 'lean' executable relative to lake or in PATH. Cannot detect stdlib path.\")\n        else:\n            try:\n                logger.debug(f\"Attempting to detect stdlib path using: {lean_exe} --print-libdir\")\n                lean_path_proc = subprocess.run(\n                    [lean_exe, '--print-libdir'],\n                    capture_output=True, text=True, check=True, timeout=10, encoding='utf-8'\n                )\n                path_candidate = lean_path_proc.stdout.strip()\n                if path_candidate and pathlib.Path(path_candidate).is_dir():\n                     std_lib_path = path_candidate\n                     logger.info(f\"Detected Lean standard library path: {std_lib_path}\")\n                else:\n                     logger.warning(f\"Lean command '{lean_exe} --print-libdir' did not return a valid directory path: '{path_candidate}'\")\n            except FileNotFoundError:\n                 logger.warning(f\"'{lean_exe}' not found when trying to detect stdlib path.\")\n            except subprocess.TimeoutExpired:\n                 logger.warning(f\"Timeout trying to detect stdlib path using '{lean_exe} --print-libdir'.\")\n            except subprocess.CalledProcessError as e:\n                 logger.warning(f\"Error running '{lean_exe} --print-libdir' (return code {e.returncode}): {e.stderr or e.stdout}\")\n            except Exception as e:\n                logger.warning(f\"Could not automatically detect Lean stdlib path using '{lean_exe} --print-libdir': {e}\")\n\n        if not std_lib_path and lean_executable_path_obj: # Fallback stdlib detection\n            try:\n                 toolchain_dir = lean_executable_path_obj.parent.parent\n                 fallback_path = toolchain_dir / \"lib\" / \"lean\"\n                 if fallback_path.is_dir():\n                     std_lib_path = str(fallback_path.resolve())\n                     logger.warning(f\"Assuming stdlib path relative to toolchain: {std_lib_path}\")\n            except NameError:\n                 logger.warning(\"Cannot attempt fallback stdlib detection due to earlier path resolution error.\")\n            except Exception as fallback_e:\n                 logger.warning(f\"Error during fallback stdlib path detection: {fallback_e}\")\n\n        if std_lib_path:\n            existing_lean_path = subprocess_env.get('LEAN_PATH')\n            separator = os.pathsep\n            if existing_lean_path: subprocess_env['LEAN_PATH'] = f\"{std_lib_path}{separator}{existing_lean_path}\"\n            else: subprocess_env['LEAN_PATH'] = std_lib_path\n            logger.debug(f\"Setting LEAN_PATH for subprocess: {subprocess_env['LEAN_PATH']}\")\n        else: logger.warning(\"Stdlib path not found or detection failed, LEAN_PATH will not include it.\")\n\n\n        # --- Set LAKE_HOME from Environment Variable (Still useful for external caching if any needed by shared lib) ---\n        persistent_cache_path = os.getenv('LEAN_AUTOMATOR_LAKE_CACHE')\n        if persistent_cache_path:\n            persistent_cache_path_obj = pathlib.Path(persistent_cache_path).resolve()\n            # Ensure directory exists (using sync os call within run_in_executor)\n            if not await loop.run_in_executor(None, persistent_cache_path_obj.is_dir):\n                logger.info(f\"Persistent Lake cache directory specified but does not exist: {persistent_cache_path_obj}\")\n                try:\n                    await loop.run_in_executor(None, functools.partial(os.makedirs, persistent_cache_path_obj, exist_ok=True))\n                    logger.info(f\"Successfully created persistent Lake cache directory: {persistent_cache_path_obj}\")\n                except OSError as e:\n                    logger.error(f\"Failed to create persistent Lake cache directory '{persistent_cache_path_obj}': {e}.\") # Removed potentially confusing part\n            subprocess_env['LAKE_HOME'] = str(persistent_cache_path_obj)\n            logger.info(f\"Setting LAKE_HOME for subprocess: {persistent_cache_path_obj}\")\n        else:\n            logger.warning(\"LEAN_AUTOMATOR_LAKE_CACHE environment variable not set. Lake will use default caching behavior.\")\n\n\n        # --- Execute Lake Build (Async Subprocess in Temp Dir) ---\n        command = [lake_executable_path, 'build', target_temp_module_name] # Build the temp module name\n        lake_process_result: Optional[subprocess.CompletedProcess] = None\n        full_output = \"\"\n        logger.info(f\"Running verification build: {' '.join(command)} in {temp_project_path_str}\")\n\n        try:\n            run_args = {\n                 'args': command, 'capture_output': True, 'text': True,\n                 'timeout': timeout_seconds, 'encoding': 'utf-8', 'errors': 'replace',\n                 'cwd': temp_project_path_str, 'check': False,\n                 'env': subprocess_env\n            }\n            lake_process_result = await loop.run_in_executor(None, functools.partial(subprocess.run, **run_args))\n\n            stdout_output = lake_process_result.stdout or \"\"\n            stderr_output = lake_process_result.stderr or \"\"\n            logger.debug(f\"Verification build return code: {lake_process_result.returncode}\")\n            if stdout_output: logger.debug(f\"Verification build stdout:\\n{stdout_output}\")\n            if stderr_output: logger.debug(f\"Verification build stderr:\\n{stderr_output}\")\n            full_output = f\"--- STDOUT ---\\n{stdout_output}\\n--- STDERR ---\\n{stderr_output}\"\n\n        except subprocess.TimeoutExpired:\n            logger.error(f\"Verification build timed out after {timeout_seconds} seconds for {unique_name}.\")\n            if target_item:\n                 target_item.update_status(ItemStatus.ERROR, f\"Timeout after {timeout_seconds}s during verification build\")\n                 await save_kb_item(target_item, client=None, db_path=effective_db_path)\n            return False, f\"Timeout after {timeout_seconds}s\"\n        except FileNotFoundError:\n             logger.error(f\"Lake executable '{lake_executable_path}' not found.\")\n             return False, f\"Lake executable not found at '{lake_executable_path}'\"\n        except Exception as e:\n            logger.exception(f\"Unexpected error running verification build subprocess for {unique_name}: {e}\")\n            if target_item:\n                 target_item.update_status(ItemStatus.ERROR, f\"Subprocess execution error: {e}\")\n                 await save_kb_item(target_item, client=None, db_path=effective_db_path)\n            return False, f\"Subprocess error during verification build: {e}\"\n\n        # --- 4. Process Verification Results ---\n        if not lake_process_result:\n             error_msg = \"Internal error: Lake process result missing after execution.\"\n             logger.error(error_msg)\n             if target_item:\n                  target_item.update_status(ItemStatus.ERROR, error_msg)\n                  await save_kb_item(target_item, client=None, db_path=effective_db_path)\n             return False, error_msg\n\n        if lake_process_result.returncode == 0:\n             # Verification successful!\n             logger.info(f\"Successfully verified {unique_name} in temporary environment.\")\n             # Update DB status first\n             target_item.update_status(ItemStatus.PROVEN, error_log=None) # Clear error log on success\n             # target_item.update_olean(None) # Ensure olean field is cleared/ignored\n             await save_kb_item(target_item, client=None, db_path=effective_db_path)\n             logger.info(f\"Database status updated to PROVEN for {unique_name}.\")\n\n             # Now, update the persistent library\n             persist_success = await _update_persistent_library(\n                 target_item, SHARED_LIB_PATH, SHARED_LIB_MODULE_NAME,\n                 lake_executable_path, timeout_seconds, loop\n             )\n             if persist_success:\n                 logger.info(f\"Successfully updated persistent library with {unique_name}.\")\n                 return True, f\"Lean code verified. Status: {ItemStatus.PROVEN.name}. Persistent library updated.\"\n             else:\n                 # Logged as error in helper, but verification itself succeeded.\n                 logger.warning(f\"Verification succeeded for {unique_name}, but failed to update/build persistent library.\")\n                 # Return success for verification, but maybe indicate the persistent lib issue.\n                 return True, f\"Lean code verified. Status: {ItemStatus.PROVEN.name}. WARNING: Persistent library update failed (see logs).\"\n\n        else: # Verification build failed\n            final_error_message = full_output.strip() if full_output else \"Unknown Lake build error (no output captured).\"\n            exit_code = lake_process_result.returncode\n            logger.warning(f\"Lean verification failed for {unique_name}. Exit code: {exit_code}\")\n            logger.debug(f\"Captured output for failed verification build of {unique_name}:\\n{full_output}\")\n\n            target_item.update_status(ItemStatus.LEAN_VALIDATION_FAILED, error_log=final_error_message)\n            # target_item.update_olean(None) # Ensure olean field is cleared/ignored\n            target_item.increment_failure_count()\n            await save_kb_item(target_item, client=None, db_path=effective_db_path)\n            return False, f\"Lean validation failed (Exit code: {exit_code}). See logs or KBItem error log for details.\"\n\n    # --- General Exception Handling for the whole process ---\n    except Exception as e:\n         logger.exception(f\"Unhandled exception during check_and_compile for {unique_name}: {e}\")\n         if target_item and isinstance(target_item, KBItem) and getattr(target_item, 'status', None) != ItemStatus.PROVEN:\n              try:\n                  target_item.update_status(ItemStatus.ERROR, f\"Unhandled exception: {e}\")\n                  # target_item.update_olean(None) # Ensure olean field is cleared/ignored\n                  await save_kb_item(target_item, client=None, db_path=effective_db_path)\n              except Exception as db_err: logger.error(f\"Failed to update item status after unhandled exception: {db_err}\")\n         return False, f\"Unhandled exception: {e}\"\n    finally:\n        # --- Cleanup Temporary Directory ---\n        try:\n            if 'temp_dir_obj' in locals() and temp_dir_obj:\n                 # Use run_in_executor for potentially blocking cleanup\n                 await loop.run_in_executor(None, temp_dir_obj.cleanup)\n                 logger.debug(f\"Successfully cleaned up temporary directory: {temp_dir_base}\")\n        except Exception as cleanup_err:\n            logger.error(f\"Error cleaning up temporary directory {temp_dir_base}: {cleanup_err}\")\n</code></pre>"},{"location":"reference/lean_processor/","title":"Lean Processor","text":"<p>Orchestrates Lean code generation and verification for Knowledge Base items.</p> <p>This module manages the process of generating formal Lean 4 code (both statement signatures and proofs) for mathematical items (<code>KBItem</code>) stored in the knowledge base. It interacts with an LLM client (<code>GeminiClient</code>) to generate the code based on LaTeX statements, informal proofs, and dependency context.</p> <p>The core logic involves: 1. Generating a Lean statement signature (<code>... := sorry</code>). 2. If the item requires proof, generating Lean proof tactics to replace <code>sorry</code>. 3. Calling the <code>lean_interaction</code> module to verify the generated code using <code>lake</code>    in a temporary environment that requires a persistent shared library. 4. Handling retries for LLM generation and potentially invoking proof repair logic. 5. Updating the <code>KBItem</code> status and content in the database based on the outcome. 6. Providing batch processing capabilities for items pending Lean processing.</p>"},{"location":"reference/lean_processor/#lean_automator.lean_processor","title":"<code>lean_automator.lean_processor</code>","text":""},{"location":"reference/lean_processor/#lean_automator.lean_processor-classes","title":"Classes","text":""},{"location":"reference/lean_processor/#lean_automator.lean_processor-functions","title":"Functions","text":""},{"location":"reference/lean_processor/#lean_automator.lean_processor.generate_and_verify_lean","title":"<code>generate_and_verify_lean(unique_name: str, client: GeminiClient, db_path: Optional[str] = None, lake_executable_path: str = 'lake', timeout_seconds: int = 120) -&gt; bool</code>  <code>async</code>","text":"<p>Generates and verifies Lean code for a KBItem using a two-step LLM process. Relies on lean_interaction.check_and_compile_item which uses a persistent shared library for dependencies. Updates persistent library on success.</p> Source code in <code>lean_automator/lean_processor.py</code> <pre><code>async def generate_and_verify_lean(\n    unique_name: str,\n    client: GeminiClient,\n    db_path: Optional[str] = None,\n    lake_executable_path: str = 'lake',\n    timeout_seconds: int = 120\n) -&gt; bool:\n    \"\"\"\n    Generates and verifies Lean code for a KBItem using a two-step LLM process.\n    Relies on lean_interaction.check_and_compile_item which uses a persistent\n    shared library for dependencies. Updates persistent library on success.\n    \"\"\"\n    start_time = time.time()\n    # Check if actual KBItem type is available, otherwise use dummy check\n    kbitem_available = KBItem is not None and not isinstance(KBItem, type('Dummy', (object,), {}))\n\n    if kbitem_available:\n        if not all([KBItem, ItemStatus, ItemType, get_kb_item_by_name, save_kb_item, lean_interaction, get_items_by_status, lean_proof_repair]):\n            logger.critical(\"Required real modules not loaded correctly. Cannot process Lean.\")\n            return False\n    else: # If using dummy types, just log a warning\n         warnings.warn(\"Running with dummy KB types. Full functionality unavailable.\", RuntimeWarning)\n\n    if not client:\n        logger.error(\"GeminiClient missing.\")\n        return False\n    if not hasattr(lean_interaction, 'check_and_compile_item'):\n         logger.critical(\"lean_interaction.check_and_compile_item missing.\")\n         return False\n    if not hasattr(lean_proof_repair, 'attempt_proof_repair'):\n         logger.critical(\"lean_proof_repair.attempt_proof_repair missing.\")\n         return False\n\n    effective_db_path = db_path or DEFAULT_DB_PATH\n\n    item = get_kb_item_by_name(unique_name, effective_db_path)\n    if not item:\n        logger.error(f\"Lean Proc: Item not found: {unique_name}\")\n        return False\n\n    # Get attributes safely in case using dummy item\n    item_status = getattr(item, 'status', None)\n    item_type = getattr(item, 'item_type', None)\n    item_lean_code = getattr(item, 'lean_code', None)\n    item_latex_statement = getattr(item, 'latex_statement', None)\n    item_plan_dependencies = getattr(item, 'plan_dependencies', [])\n    item_latex_proof = getattr(item, 'latex_proof', None)\n\n    if item_status == ItemStatus.PROVEN:\n        logger.info(f\"Lean Proc: Already PROVEN: {unique_name}\")\n        return True\n\n    trigger_statuses = {ItemStatus.LATEX_ACCEPTED, ItemStatus.PENDING_LEAN, ItemStatus.LEAN_VALIDATION_FAILED}\n    if item_status not in trigger_statuses:\n        logger.warning(f\"Lean Proc: Item {unique_name} not in a trigger status. Status: {item_status.name if item_status else 'None'}\")\n        return False\n\n    if not item_latex_statement:\n         logger.error(f\"Lean Proc: Missing latex_statement for {unique_name}\")\n         if hasattr(item, 'update_status'):\n             item.update_status(ItemStatus.ERROR, \"Missing latex_statement for Lean generation.\")\n             await save_kb_item(item, client=None, db_path=effective_db_path)\n         return False\n\n    proof_required = item_type.requires_proof() if item_type else False\n    if not proof_required:\n        logger.info(f\"Lean Proc: No proof required for {unique_name} ({item_type.name if item_type else 'N/A'}). Marking PROVEN.\")\n        if not item_lean_code:\n             logger.warning(f\"Lean Proc: Non-provable item {unique_name} has no Lean code. Requires statement generation first.\")\n             # Need statement generation even if no proof needed\n             # Fall through to statement generation\n             pass\n        else:\n            # Already has code, no proof needed, mark proven\n            if hasattr(item, 'update_status'):\n                item.update_status(ItemStatus.PROVEN)\n                await save_kb_item(item, client=None, db_path=effective_db_path)\n            return True\n\n    dependency_items: List[KBItem] = []\n    valid_dep_statuses = {ItemStatus.PROVEN, ItemStatus.AXIOM_ACCEPTED, ItemStatus.DEFINITION_ADDED}\n    logger.debug(f\"Checking dependencies for {unique_name}: {item_plan_dependencies}\")\n    all_deps_ready = True\n    for dep_name in item_plan_dependencies:\n        dep_item = get_kb_item_by_name(dep_name, effective_db_path)\n        dep_status = getattr(dep_item, 'status', None)\n        dep_code_exists = bool(getattr(dep_item, 'lean_code', None))\n\n        if not dep_item or dep_status not in valid_dep_statuses or not dep_code_exists:\n            logger.error(f\"Dependency '{dep_name}' for '{unique_name}' not ready (status={dep_status.name if dep_status else 'MISSING'}, code_exists={dep_code_exists}). Cannot proceed.\")\n            if hasattr(item, 'update_status'):\n                item.update_status(ItemStatus.ERROR, f\"Dependency '{dep_name}' not ready.\")\n                await save_kb_item(item, client=None, db_path=effective_db_path)\n            all_deps_ready = False\n            break # No need to check further dependencies\n        dependency_items.append(dep_item)\n\n    if not all_deps_ready:\n        return False # Dependency check failed\n\n    logger.debug(f\"All {len(dependency_items)} dependencies for {unique_name} are assumed available in shared library.\")\n\n    # No longer need to generate import block here - lean_interaction handles it for the target file\n    # dependency_import_block = _generate_dependency_import_block(dependency_items) # Removed\n    # logger.debug(f\"Generated import block for {unique_name}:\\n{dependency_import_block}\") # Removed\n\n    original_lean_statement_shell = item_lean_code\n    statement_error_feedback = None\n\n    needs_statement_generation = not item_lean_code or item_status in {ItemStatus.LATEX_ACCEPTED, ItemStatus.PENDING_LEAN}\n\n    if needs_statement_generation:\n        logger.info(f\"Starting Lean statement generation phase for {unique_name}\")\n        statement_accepted = False\n        for attempt in range(LEAN_STATEMENT_MAX_ATTEMPTS):\n            logger.info(f\"Lean Statement Attempt {attempt + 1}/{LEAN_STATEMENT_MAX_ATTEMPTS} for {unique_name}\")\n            if hasattr(item, 'update_status'):\n                item.update_status(ItemStatus.LEAN_GENERATION_IN_PROGRESS, f\"Statement attempt {attempt + 1}\")\n                await save_kb_item(item, client=None, db_path=effective_db_path)\n\n            # Fetch potentially updated dependency items? Usually not needed for statement gen.\n            raw_response = await _call_lean_statement_generator(item, dependency_items, statement_error_feedback, client)\n            parsed_header = _extract_lean_header(raw_response)\n\n            if not parsed_header:\n                 logger.warning(f\"Failed to parse Lean header on attempt {attempt + 1}.\")\n                 statement_error_feedback = f\"LLM output did not contain a valid Lean header (attempt {attempt+1}). Raw response: {repr(raw_response[:500])}\"\n                 if hasattr(item, 'raw_ai_response'): item.raw_ai_response = raw_response\n                 if hasattr(item, 'generation_prompt'): item.generation_prompt = \"Statement Gen Prompt (see logs/code)\" # Placeholder\n                 await save_kb_item(item, client=None, db_path=effective_db_path)\n                 continue\n\n            logger.info(f\"Lean statement shell generated successfully for {unique_name}\")\n            original_lean_statement_shell = parsed_header\n            if hasattr(item, 'lean_code'): item.lean_code = original_lean_statement_shell\n            if hasattr(item, 'generation_prompt'): item.generation_prompt = \"Statement Gen Prompt (see logs/code)\" # Placeholder\n            if hasattr(item, 'raw_ai_response'): item.raw_ai_response = raw_response\n            await save_kb_item(item, client=None, db_path=effective_db_path)\n            statement_accepted = True\n            break # Exit loop on success\n\n        if not statement_accepted:\n            logger.error(f\"Failed to generate valid Lean statement shell for {unique_name} after {LEAN_STATEMENT_MAX_ATTEMPTS} attempts.\")\n            if hasattr(item, 'update_status'):\n                item.update_status(ItemStatus.ERROR, f\"Failed Lean statement generation after {LEAN_STATEMENT_MAX_ATTEMPTS} attempts. Last feedback: {statement_error_feedback}\")\n                if hasattr(item, 'lean_error_log'): item.lean_error_log = statement_error_feedback\n                await save_kb_item(item, client=None, db_path=effective_db_path)\n            return False\n    else:\n         # Already has lean code, use it as the shell\n         logger.info(f\"Skipping Lean statement generation for {unique_name}, using existing shell.\")\n         original_lean_statement_shell = item_lean_code\n         if not original_lean_statement_shell or \":= sorry\" not in original_lean_statement_shell:\n             logger.error(f\"Existing lean_code for {unique_name} is invalid or missing ':= sorry'. Cannot proceed to proof generation.\")\n             if hasattr(item, 'update_status'):\n                 item.update_status(ItemStatus.ERROR, \"Invalid existing lean_code shell for proof generation.\")\n                 await save_kb_item(item, client=None, db_path=effective_db_path)\n             return False\n\n    # Handle non-provable items again after potential statement generation\n    if not proof_required:\n        logger.info(f\"Lean Proc: Statement generated/present for non-provable {unique_name}. Marking PROVEN.\")\n        # Re-fetch item to ensure we have the latest version before updating status\n        item_final = get_kb_item_by_name(unique_name, effective_db_path)\n        if item_final and getattr(item_final, 'status', None) != ItemStatus.PROVEN:\n            if hasattr(item_final, 'update_status'):\n                item_final.update_status(ItemStatus.PROVEN)\n                await save_kb_item(item_final, client=None, db_path=effective_db_path)\n        elif not item_final:\n             logger.error(f\"Item {unique_name} disappeared before final PROVEN update for non-provable.\")\n        return True # Return True as statement exists and no proof needed\n\n\n    # --- Proof Generation Phase ---\n    logger.info(f\"Starting Lean proof generation phase for {unique_name}\")\n    lean_verification_success = False\n\n    # Fetch item state just before the loop begins\n    item_before_proof_loop = get_kb_item_by_name(unique_name, effective_db_path)\n    if not item_before_proof_loop or not original_lean_statement_shell:\n         logger.error(f\"Cannot proceed to proof generation: Item {unique_name} missing or shell invalid before proof loop.\")\n         return False\n\n    # Clear error log if status is not already LEAN_VALIDATION_FAILED\n    if getattr(item_before_proof_loop, 'status', None) != ItemStatus.LEAN_VALIDATION_FAILED:\n         if hasattr(item_before_proof_loop, 'lean_error_log') and getattr(item_before_proof_loop, 'lean_error_log', None) is not None:\n             logger.debug(f\"Clearing previous error log for {unique_name} as status is {item_before_proof_loop.status.name if item_before_proof_loop.status else 'None'}\")\n             item_before_proof_loop.lean_error_log = None\n             await save_kb_item(item_before_proof_loop, client=None, db_path=effective_db_path)\n\n\n    for attempt in range(LEAN_PROOF_MAX_ATTEMPTS):\n        logger.info(f\"Lean Proof Attempt {attempt + 1}/{LEAN_PROOF_MAX_ATTEMPTS} for {unique_name}\")\n        # Fetch the latest state within the loop for error log feedback\n        current_item_state = get_kb_item_by_name(unique_name, effective_db_path)\n        if not current_item_state:\n            logger.error(f\"Item {unique_name} disappeared mid-proof attempts!\")\n            return False # Critical error\n\n        # Update status to indicate generation is in progress\n        if hasattr(current_item_state, 'update_status'):\n            current_item_state.update_status(ItemStatus.LEAN_GENERATION_IN_PROGRESS, f\"Proof attempt {attempt + 1}\")\n            await save_kb_item(current_item_state, client=None, db_path=effective_db_path)\n\n        # Get attributes needed for the call safely\n        current_latex_proof = getattr(current_item_state, 'latex_proof', None)\n        current_item_type_name = getattr(getattr(current_item_state, 'item_type', None), 'name', 'UNKNOWN_TYPE')\n        current_lean_error_log = getattr(current_item_state, 'lean_error_log', None)\n\n\n        raw_llm_response = await _call_lean_proof_generator(\n            lean_statement_shell=original_lean_statement_shell,\n            latex_proof=current_latex_proof,\n            unique_name=unique_name, # Use unique_name directly\n            item_type_name=current_item_type_name,\n            dependencies=dependency_items, # Pass fetched dependencies for context\n            lean_error_log=current_lean_error_log,\n            client=client\n        )\n\n        if not raw_llm_response:\n            logger.warning(f\"Lean proof generator returned no response on attempt {attempt + 1}\")\n            if attempt + 1 == LEAN_PROOF_MAX_ATTEMPTS:\n                 # Update status on final attempt failure\n                 final_item_state = get_kb_item_by_name(unique_name, effective_db_path)\n                 if final_item_state and hasattr(final_item_state, 'update_status'):\n                     final_item_state.update_status(ItemStatus.ERROR, f\"LLM failed to generate proof on final attempt {attempt + 1}\")\n                     if hasattr(final_item_state, 'lean_error_log'): final_item_state.lean_error_log = \"LLM provided no output.\"\n                     await save_kb_item(final_item_state, client=None, db_path=effective_db_path)\n            continue # Try next attempt\n\n        generated_lean_code_from_llm = _extract_lean_code(raw_llm_response)\n        if not generated_lean_code_from_llm:\n            logger.warning(f\"Failed to parse Lean code from proof generator on attempt {attempt + 1}.\")\n            item_parse_fail = get_kb_item_by_name(unique_name, effective_db_path)\n            if item_parse_fail:\n                if hasattr(item_parse_fail, 'raw_ai_response'): item_parse_fail.raw_ai_response = raw_llm_response\n                error_message = f\"LLM output parsing failed on proof attempt {attempt + 1}. Raw: {repr(raw_llm_response[:500])}\"\n                if hasattr(item_parse_fail, 'lean_error_log'): item_parse_fail.lean_error_log = error_message\n                if hasattr(item_parse_fail, 'generation_prompt'): item_parse_fail.generation_prompt = \"Proof Gen Prompt (see logs/code)\" # Placeholder\n                if attempt + 1 == LEAN_PROOF_MAX_ATTEMPTS:\n                    if hasattr(item_parse_fail, 'update_status'): item_parse_fail.update_status(ItemStatus.ERROR, f\"LLM output parsing failed on final Lean proof attempt\")\n                await save_kb_item(item_parse_fail, client=None, db_path=effective_db_path)\n            continue # Try next attempt\n\n        # Log the generated code before verification\n        logger.info(f\"--- LLM Generated Code (Attempt {attempt + 1}) ---\")\n        logger.info(f\"\\n{generated_lean_code_from_llm}\\n\")\n        logger.info(f\"--- End LLM Generated Code (Attempt {attempt + 1}) ---\")\n\n        # --- Prepare item for verification ---\n        # The generated code should contain the full definition/theorem including proof\n        # No need to prepend import block here - lean_interaction._create_temp_env... handles imports for the target file\n        final_lean_code = generated_lean_code_from_llm\n\n        item_to_verify = get_kb_item_by_name(unique_name, effective_db_path)\n        if not item_to_verify:\n            logger.error(f\"Item {unique_name} vanished before Lean verification attempt {attempt + 1}!\")\n            return False # Critical error\n\n        # Update item with the code to be verified\n        if hasattr(item_to_verify, 'lean_code'): item_to_verify.lean_code = final_lean_code\n        if hasattr(item_to_verify, 'generation_prompt'): item_to_verify.generation_prompt = \"Proof Gen Prompt (see logs/code)\" # Placeholder\n        if hasattr(item_to_verify, 'raw_ai_response'): item_to_verify.raw_ai_response = raw_llm_response\n        if hasattr(item_to_verify, 'update_status'): item_to_verify.update_status(ItemStatus.LEAN_VALIDATION_PENDING)\n        await save_kb_item(item_to_verify, client=None, db_path=effective_db_path)\n\n        logger.info(f\"Calling lean_interaction.check_and_compile_item for {unique_name} (Proof Attempt {attempt + 1})\")\n\n        try:\n            # Call the updated lean_interaction function\n            # It now handles requiring the shared library and updating it on success\n            verified, message = await lean_interaction.check_and_compile_item(\n                unique_name=unique_name,\n                db_path=effective_db_path,\n                lake_executable_path=lake_executable_path,\n                timeout_seconds=timeout_seconds\n                # Pass shared lib info if check_and_compile_item needs it explicitly\n                # (currently reads from env vars/constants within lean_interaction)\n            )\n\n            if verified:\n                logger.info(f\"Successfully verified Lean code for: {unique_name} on attempt {attempt + 1}.\")\n                # check_and_compile_item now handles DB status update to PROVEN and persistent lib update\n                lean_verification_success = True\n                break # Exit loop on success\n            else:\n                # Verification failed\n                logger.warning(f\"Verification failed for {unique_name} on proof attempt {attempt + 1}. Message: {message[:500]}...\")\n                # check_and_compile_item already updated status to LEAN_VALIDATION_FAILED and saved error log.\n\n                # Fetch the item again to get the error log for the next LLM attempt or final state\n                item_after_fail = get_kb_item_by_name(unique_name, effective_db_path)\n                if item_after_fail:\n                     latest_error_log = getattr(item_after_fail, 'lean_error_log', None)\n                     if latest_error_log:\n                         logger.warning(f\"--- Lean Error Log (Attempt {attempt + 1}) ---\")\n                         logger.warning(f\"\\n{latest_error_log}\\n\")\n                         logger.warning(f\"--- End Lean Error Log (Attempt {attempt + 1}) ---\")\n                     else: # Should not happen if check_and_compile saves log on failure\n                         logger.warning(f\"No specific error log captured in DB for attempt {attempt + 1}, message: {message}\")\n\n                     # --- Optional: Proof Repair ---\n                     # Consider if repair logic should be attempted here.\n                     # Current repair logic is minimal/disabled.\n                     if hasattr(item_after_fail, 'lean_code') and latest_error_log:\n                         fix_applied, _ = lean_proof_repair.attempt_proof_repair(\n                             item_after_fail.lean_code,\n                             latest_error_log\n                         )\n                         if fix_applied:\n                              logger.info(f\"Automated proof repair applied for {unique_name}. Re-verification *not* implemented in this loop.\")\n                              # If repair modifies code, the next LLM attempt might get stale code?\n                              # Or should we re-verify immediately after repair? Adds complexity.\n                              # For now, just log that repair was applied. The next LLM attempt\n                              # will use the error log from the *original* failed verification.\n                         else:\n                              logger.debug(f\"No automated fix applied for {unique_name} on attempt {attempt + 1}.\")\n                     # --- End Optional Proof Repair ---\n\n                else: # Should not happen if item exists\n                     logger.error(f\"Item {unique_name} vanished after failed verification attempt {attempt + 1}!\")\n                     return False\n\n                # Continue to the next LLM attempt loop iteration (if attempts remain)\n                continue # Go to next attempt\n\n        except Exception as verify_err:\n            logger.exception(f\"check_and_compile_item failed unexpectedly for {unique_name} on proof attempt {attempt + 1}: {verify_err}\")\n            item_err_state = get_kb_item_by_name(unique_name, effective_db_path)\n            if item_err_state and hasattr(item_err_state, 'update_status'):\n                err_log_message = f\"Verification system crashed: {verify_err}\"\n                item_err_state.update_status(ItemStatus.ERROR, f\"Lean verification system error: {verify_err}\")\n                if hasattr(item_err_state, 'lean_error_log'): item_err_state.lean_error_log = err_log_message\n                if hasattr(item_err_state, 'increment_failure_count'): item_err_state.increment_failure_count()\n                await save_kb_item(item_err_state, client=None, db_path=effective_db_path)\n                # Log error if available\n                if getattr(item_err_state, 'lean_error_log', None):\n                     logger.warning(f\"--- Lean Error Log (Verification Crash - Attempt {attempt + 1}) ---\")\n                     logger.warning(f\"\\n{item_err_state.lean_error_log}\\n\")\n                     logger.warning(f\"--- End Lean Error Log ---\")\n\n            continue # Try next attempt\n\n\n    # --- Loop Finished ---\n    end_time = time.time()\n    duration = end_time - start_time\n\n    if lean_verification_success:\n        logger.info(f\"Lean processing SUCCEEDED for {unique_name} in {duration:.2f} seconds.\")\n        return True\n    else:\n        logger.error(f\"Failed to generate/verify Lean proof for {unique_name} after all attempts. Total time: {duration:.2f} seconds.\")\n        # Final status should be LEAN_VALIDATION_FAILED or ERROR, set by check_and_compile or exception handling\n        # Re-fetch just to double-check status if needed for logging/return value, but don't change DB state here.\n        final_item = get_kb_item_by_name(unique_name, effective_db_path)\n        if final_item and getattr(final_item, 'status', None) == ItemStatus.PROVEN:\n             # This case indicates maybe check_and_compile succeeded but loop logic was wrong? Unlikely with current structure.\n            logger.warning(f\"Item {unique_name} is PROVEN despite loop indicating failure. Assuming success.\")\n            return True\n        # Otherwise, failure is the correct outcome\n        return False\n</code></pre>"},{"location":"reference/lean_processor/#lean_automator.lean_processor.process_pending_lean_items","title":"<code>process_pending_lean_items(client: GeminiClient, db_path: Optional[str] = None, limit: Optional[int] = None, process_statuses: Optional[List[ItemStatus]] = None, **kwargs)</code>  <code>async</code>","text":"<p>Processes items in specified statuses using generate_and_verify_lean.</p> Source code in <code>lean_automator/lean_processor.py</code> <pre><code>async def process_pending_lean_items(\n        client: GeminiClient,\n        db_path: Optional[str] = None,\n        limit: Optional[int] = None,\n        process_statuses: Optional[List[ItemStatus]] = None,\n        **kwargs # Pass other args like lake path, timeout\n    ):\n    \"\"\"Processes items in specified statuses using generate_and_verify_lean.\"\"\"\n    # Check if actual KBItem type is available\n    kbitem_available = KBItem is not None and not isinstance(KBItem, type('Dummy', (object,), {}))\n    if kbitem_available:\n        if not all([ItemStatus, ItemType, get_items_by_status, get_kb_item_by_name, save_kb_item, lean_proof_repair]):\n             logger.critical(\"KB Storage components or proof repair module not loaded correctly. Cannot batch process Lean items.\")\n             return\n    else:\n        warnings.warn(\"Running batch process with dummy KB types. Full functionality unavailable.\", RuntimeWarning)\n\n\n    effective_db_path = db_path or DEFAULT_DB_PATH\n    if process_statuses is None:\n         process_statuses = {ItemStatus.PENDING_LEAN, ItemStatus.LATEX_ACCEPTED, ItemStatus.LEAN_VALIDATION_FAILED}\n    else:\n         process_statuses = set(process_statuses) # Ensure it's a set\n\n\n    processed_count = 0\n    success_count = 0\n    fail_count = 0\n    items_to_process_names = []\n\n    logger.info(f\"Querying for items with statuses: {[s.name for s in process_statuses]} for Lean processing.\")\n    for status in process_statuses:\n        try:\n            items_gen = get_items_by_status(status, effective_db_path)\n            count_for_status = 0\n            for item in items_gen:\n                # Check limit\n                if limit is not None and len(items_to_process_names) &gt;= limit:\n                    logger.info(f\"Reached processing limit of {limit} items.\")\n                    break # Break inner loop\n\n                item_unique_name = getattr(item, 'unique_name', None)\n                if not item_unique_name: continue # Skip items without a name\n\n                # Handle non-provable items needing only statement generation\n                item_type = getattr(item, 'item_type', None)\n                item_status = getattr(item, 'status', None)\n                item_lean_code = getattr(item, 'lean_code', None)\n                needs_proof = item_type.requires_proof() if item_type else True # Assume proof needed if type unknown\n\n                if not needs_proof and item_status == ItemStatus.LATEX_ACCEPTED:\n                     logger.info(f\"Found non-provable item {item_unique_name} ({item_type.name if item_type else 'N/A'}) with status LATEX_ACCEPTED.\")\n                     if item_lean_code:\n                         logger.info(f\"Item {item_unique_name} has code, marking PROVEN.\")\n                         if hasattr(item, 'update_status'):\n                             item.update_status(ItemStatus.PROVEN)\n                             await save_kb_item(item, client=None, db_path=effective_db_path)\n                         # Don't add to processing list\n                         continue\n                     else:\n                         logger.info(f\"Item {item_unique_name} needs statement generation. Adding to process list.\")\n                         # Fall through to add to list below\n\n                # Add eligible items to list if not already present\n                if item_unique_name not in items_to_process_names:\n                    items_to_process_names.append(item_unique_name)\n                    count_for_status += 1\n\n            logger.debug(f\"Found {count_for_status} potential items with status {status.name}\")\n            # Check limit again after iterating through a status\n            if limit is not None and len(items_to_process_names) &gt;= limit:\n                break # Break outer loop\n\n        except Exception as e:\n            logger.error(f\"Failed to retrieve items with status {status.name}: {e}\")\n\n    if not items_to_process_names:\n         logger.info(\"No items found requiring Lean proof processing in the specified statuses.\")\n         return\n\n    logger.info(f\"Found {len(items_to_process_names)} unique items for Lean processing.\")\n\n    # Process items one by one\n    for unique_name in items_to_process_names:\n        # Re-fetch item state before processing to ensure it's still eligible\n        current_item_state = get_kb_item_by_name(unique_name, effective_db_path)\n        if not current_item_state:\n             logger.warning(f\"Skipping {unique_name} as it could not be re-fetched before processing.\")\n             continue\n\n        item_status = getattr(current_item_state, 'status', None)\n        item_type = getattr(current_item_state, 'item_type', None)\n        item_lean_code = getattr(current_item_state, 'lean_code', None)\n        needs_proof = item_type.requires_proof() if item_type else True\n\n        # Check eligibility again\n        is_eligible_status = item_status in process_statuses\n        # Needs processing if it requires proof OR if it doesn't require proof but lacks code\n        needs_processing = needs_proof or (not needs_proof and not item_lean_code)\n\n        if not is_eligible_status or not needs_processing:\n             logger.info(f\"Skipping {unique_name} as its status ({item_status.name if item_status else 'None'}) or state changed, making it ineligible.\")\n             continue\n\n        logger.info(f\"--- Processing Lean for: {unique_name} (ID: {getattr(current_item_state, 'id', 'N/A')}, Status: {item_status.name if item_status else 'None'}) ---\")\n        try:\n            # Call the main processing function\n            success = await generate_and_verify_lean(\n                unique_name, client, effective_db_path, **kwargs\n            )\n            if success:\n                success_count += 1\n            else:\n                fail_count += 1\n        except Exception as e:\n             # Catch unexpected errors during the processing of a single item\n             logger.exception(f\"Critical error during generate_and_verify_lean for {unique_name}: {e}\")\n             fail_count += 1\n             # Attempt to mark the item with an error status in the DB\n             try:\n                  err_item = get_kb_item_by_name(unique_name, effective_db_path)\n                  if err_item and getattr(err_item, 'status', None) != ItemStatus.PROVEN:\n                     if hasattr(err_item, 'update_status'):\n                         err_item.update_status(ItemStatus.ERROR, f\"Batch Lean processing crashed: {e}\")\n                         await save_kb_item(err_item, client=None, db_path=effective_db_path)\n             except Exception as save_err:\n                  logger.error(f\"Failed to save ERROR status for {unique_name} after batch processing crash: {save_err}\")\n\n        processed_count += 1\n        logger.info(f\"--- Finished processing Lean for {unique_name} ---\")\n\n    logger.info(f\"Lean Batch Processing Complete. Total Processed: {processed_count}, Succeeded: {success_count}, Failed: {fail_count}\")\n</code></pre>"},{"location":"reference/lean_proof_repair/","title":"Lean Proof Repair","text":"<p>Attempts heuristic automated repairs for common Lean compilation errors.</p> <p>This module provides functionality to analyze Lean compilation error logs and attempt simple, pattern-based repairs on the corresponding Lean code. The goal is to automatically fix common, easily identifiable issues that might arise from LLM code generation, potentially reducing the number of LLM retry cycles needed.</p> <p>Note: Currently, specific repair handlers (like for 'no goals to be solved') are disabled, and the main function <code>attempt_proof_repair</code> will always return indicating no fix was applied. Future handlers for other error patterns can be added here.</p>"},{"location":"reference/lean_proof_repair/#lean_automator.lean_proof_repair","title":"<code>lean_automator.lean_proof_repair</code>","text":""},{"location":"reference/lean_proof_repair/#lean_automator.lean_proof_repair-functions","title":"Functions","text":""},{"location":"reference/lean_proof_repair/#lean_automator.lean_proof_repair.attempt_proof_repair","title":"<code>attempt_proof_repair(lean_code: str, error_log: str) -&gt; Tuple[bool, str]</code>","text":"<p>Attempts to automatically fix known, simple errors in generated Lean code based on the provided error log. Currently, known handlers are disabled.</p> <p>Parameters:</p> Name Type Description Default <code>lean_code</code> <code>str</code> <p>The Lean code string that failed compilation.</p> required <code>error_log</code> <code>str</code> <p>The stderr/stdout captured from the failed lean/lake build.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A tuple: (fix_attempted_and_applied: bool, resulting_code: str).</p> <code>str</code> <p>Always returns (False, original_code) in the current configuration.</p> Source code in <code>lean_automator/lean_proof_repair.py</code> <pre><code>def attempt_proof_repair(lean_code: str, error_log: str) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Attempts to automatically fix known, simple errors in generated Lean code\n    based on the provided error log. Currently, known handlers are disabled.\n\n    Args:\n        lean_code: The Lean code string that failed compilation.\n        error_log: The stderr/stdout captured from the failed lean/lake build.\n\n    Returns:\n        A tuple: (fix_attempted_and_applied: bool, resulting_code: str).\n        Always returns (False, original_code) in the current configuration.\n    \"\"\"\n    logger.debug(\"Attempting automated proof repair...\")\n    original_code = lean_code # Keep original for comparison\n\n    if not lean_code or not error_log:\n        logger.debug(\"No code or error log provided for repair.\")\n        return False, original_code\n\n    # --- Handler 1: Only \"no goals to be solved\" errors (DISABLED) ---\n    # This handler was disabled because simple local repairs (delete line or\n    # replace with 'done') proved insufficient or unreliable for the underlying\n    # issue where a preceding tactic implicitly solved the goal. Relying on\n    # LLM retry with the original error log is preferred for this case.\n    \"\"\"\n    if _is_only_no_goals_error(error_log):\n        logger.info(\"Detected pattern where all Lean source errors are 'no goals to be solved'. Attempting fix...\")\n        try:\n            modified_code = _fix_no_goals_error(lean_code, error_log) # Using replace with 'done' version\n            if modified_code != original_code:\n                 logger.info(\"Applied fix for 'no goals to be solved'.\")\n                 if not modified_code.strip():\n                      logger.error(\"Automated fix resulted in empty code. Reverting.\")\n                      return False, original_code\n                 return True, modified_code\n            else:\n                 logger.warning(\"Identified 'no goals' pattern, but generated code was unchanged by fix function (potentially failed line number extraction?).\")\n                 return False, original_code # No effective change made\n        except Exception as e:\n            logger.exception(f\"Error during 'no goals' fix execution: {e}\", exc_info=True)\n            return False, original_code # Fix failed, return original\n    \"\"\"\n\n    # --- Add elif blocks here for future error handlers ---\n    # Example:\n    # elif _is_some_other_fixable_error(error_log):\n    #    logger.info(\"Detected other fixable pattern...\")\n    #    try:\n    #        modified_code = _fix_other_error(lean_code, error_log)\n    #        if modified_code != original_code:\n    #            logger.info(\"Applied fix for other error.\")\n    #            return True, modified_code\n    #        else:\n    #            return False, original_code\n    #    except Exception as e:\n    #        logger.exception(f\"Error during other fix execution: {e}\")\n    #        return False, original_code\n\n    # --- Default: No fix applied ---\n    # This is reached if no enabled handlers match the error log.\n    logger.debug(\"No enabled/matching fixable error pattern detected.\")\n    return False, original_code\n</code></pre>"},{"location":"reference/llm_call/","title":"LLM Call","text":"<p>Provides a client for interacting with Google's Gemini API.</p> <p>This module defines classes and functions to facilitate communication with Google's Gemini large language models (LLMs) for both text generation and embedding creation. It includes features like asynchronous API calls, automatic retries with exponential backoff for transient errors, and integrated cost tracking based on model usage (tokens/units).</p>"},{"location":"reference/llm_call/#lean_automator.llm_call","title":"<code>lean_automator.llm_call</code>","text":""},{"location":"reference/llm_call/#lean_automator.llm_call-classes","title":"Classes","text":""},{"location":"reference/llm_call/#lean_automator.llm_call.ModelUsageStats","title":"<code>ModelUsageStats</code>  <code>dataclass</code>","text":"<p>Stores usage statistics for a specific model.</p> Source code in <code>lean_automator/llm_call.py</code> <pre><code>@dataclass\nclass ModelUsageStats:\n    \"\"\"Stores usage statistics for a specific model.\"\"\"\n    calls: int = 0\n    prompt_tokens: int = 0 # Represents input tokens/units\n    completion_tokens: int = 0 # Represents output tokens/units (often 0 for embeddings)\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.ModelCostInfo","title":"<code>ModelCostInfo</code>  <code>dataclass</code>","text":"<p>Stores cost per MILLION units (tokens/chars) for a specific model.</p> Source code in <code>lean_automator/llm_call.py</code> <pre><code>@dataclass\nclass ModelCostInfo:\n    \"\"\"Stores cost per MILLION units (tokens/chars) for a specific model.\"\"\"\n    input_cost_per_million_units: float\n    output_cost_per_million_units: float\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiCostTracker","title":"<code>GeminiCostTracker</code>","text":"<p>Tracks API call counts, token usage, and estimated costs for Gemini models, using costs specified per million units. Handles generative and embedding models.</p> Source code in <code>lean_automator/llm_call.py</code> <pre><code>class GeminiCostTracker:\n    \"\"\"\n    Tracks API call counts, token usage, and estimated costs for Gemini models,\n    using costs specified per million units. Handles generative and embedding models.\n    \"\"\"\n    def __init__(self, model_costs_json: Optional[str] = None):\n        \"\"\"\n        Initializes the tracker.\n\n        Args:\n            model_costs_json: A JSON string mapping model names to their costs per million units.\n                              If None, reads from GEMINI_MODEL_COSTS environment variable.\n                              Example: '{\"gemini-1.5-flash-latest\": {\"input\": 0.35, \"output\": 0.70}, \"models/text-embedding-004\": {\"input\": 0.10, \"output\": 0.0}}'\n        \"\"\"\n        effective_costs_json = model_costs_json if model_costs_json is not None else os.getenv('GEMINI_MODEL_COSTS', FALLBACK_MODEL_COSTS_JSON)\n        self._usage_stats: Dict[str, ModelUsageStats] = {}\n        self._model_costs: Dict[str, ModelCostInfo] = {}\n        self._parse_model_costs(effective_costs_json)\n\n    def _parse_model_costs(self, json_string: str):\n        \"\"\"Parses the model costs JSON string (expecting cost per million units).\"\"\"\n        try:\n            costs_dict = json.loads(json_string)\n            for model, costs in costs_dict.items():\n                # Allow 'output' to be missing or 0 for embedding models\n                if isinstance(costs, dict) and 'input' in costs:\n                    input_cost = float(costs['input'])\n                    # Default output cost to 0 if not specified\n                    output_cost = float(costs.get('output', 0.0))\n                    self._model_costs[model] = ModelCostInfo(\n                        input_cost_per_million_units=input_cost,\n                        output_cost_per_million_units=output_cost\n                    )\n                else:\n                    warnings.warn(f\"Invalid cost format for model '{model}' in GEMINI_MODEL_COSTS. Expected at least {{'input': float}}. Found: {costs}\")\n        except json.JSONDecodeError:\n            warnings.warn(\"Failed to parse GEMINI_MODEL_COSTS JSON string. Costs will not be tracked accurately.\")\n        except Exception as e:\n            warnings.warn(f\"Error processing GEMINI_MODEL_COSTS: {e}\")\n\n    def record_usage(self, model: str, input_units: int, output_units: int):\n        \"\"\"Records a successful API call and its unit usage.\"\"\"\n        if model not in self._usage_stats:\n            self._usage_stats[model] = ModelUsageStats()\n\n        stats = self._usage_stats[model]\n        stats.calls += 1\n        stats.prompt_tokens += input_units # Map input_units -&gt; prompt_tokens\n        stats.completion_tokens += output_units # Map output_units -&gt; completion_tokens\n\n    def get_total_cost(self) -&gt; float:\n        \"\"\"Calculates the estimated total cost based on recorded usage and known model costs per million units.\"\"\"\n        total_cost = 0.0\n        for model, stats in self._usage_stats.items():\n            if model in self._model_costs:\n                costs = self._model_costs[model]\n                total_cost += (stats.prompt_tokens / 1_000_000.0) * costs.input_cost_per_million_units + \\\n                              (stats.completion_tokens / 1_000_000.0) * costs.output_cost_per_million_units\n            else:\n                warnings.warn(f\"Cost information missing for model '{model}'. Usage for this model is not included in total cost.\")\n        return total_cost\n\n    def get_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns a summary dictionary of usage and estimated costs.\"\"\"\n        total_estimated_cost = self.get_total_cost() # Ensure warnings are potentially triggered\n        summary: Dict[str, Any] = {\n            \"total_estimated_cost\": total_estimated_cost,\n            \"usage_by_model\": {}\n        }\n        for model, stats in self._usage_stats.items():\n            model_summary = {\n                \"calls\": stats.calls,\n                \"input_units\": stats.prompt_tokens,\n                \"output_units\": stats.completion_tokens, # Usually 0 for embeddings\n                \"estimated_cost\": 0.0\n            }\n            if model in self._model_costs:\n                costs = self._model_costs[model]\n                model_summary[\"estimated_cost\"] = (stats.prompt_tokens / 1_000_000.0) * costs.input_cost_per_million_units + \\\n                                                  (stats.completion_tokens / 1_000_000.0) * costs.output_cost_per_million_units\n            else:\n                 model_summary[\"estimated_cost\"] = \"Unknown (cost data missing)\"\n\n            summary[\"usage_by_model\"][model] = model_summary\n        return summary\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiCostTracker-functions","title":"Functions","text":""},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiCostTracker.__init__","title":"<code>__init__(model_costs_json: Optional[str] = None)</code>","text":"<p>Initializes the tracker.</p> <p>Parameters:</p> Name Type Description Default <code>model_costs_json</code> <code>Optional[str]</code> <p>A JSON string mapping model names to their costs per million units.               If None, reads from GEMINI_MODEL_COSTS environment variable.               Example: '{\"gemini-1.5-flash-latest\": {\"input\": 0.35, \"output\": 0.70}, \"models/text-embedding-004\": {\"input\": 0.10, \"output\": 0.0}}'</p> <code>None</code> Source code in <code>lean_automator/llm_call.py</code> <pre><code>def __init__(self, model_costs_json: Optional[str] = None):\n    \"\"\"\n    Initializes the tracker.\n\n    Args:\n        model_costs_json: A JSON string mapping model names to their costs per million units.\n                          If None, reads from GEMINI_MODEL_COSTS environment variable.\n                          Example: '{\"gemini-1.5-flash-latest\": {\"input\": 0.35, \"output\": 0.70}, \"models/text-embedding-004\": {\"input\": 0.10, \"output\": 0.0}}'\n    \"\"\"\n    effective_costs_json = model_costs_json if model_costs_json is not None else os.getenv('GEMINI_MODEL_COSTS', FALLBACK_MODEL_COSTS_JSON)\n    self._usage_stats: Dict[str, ModelUsageStats] = {}\n    self._model_costs: Dict[str, ModelCostInfo] = {}\n    self._parse_model_costs(effective_costs_json)\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiCostTracker.record_usage","title":"<code>record_usage(model: str, input_units: int, output_units: int)</code>","text":"<p>Records a successful API call and its unit usage.</p> Source code in <code>lean_automator/llm_call.py</code> <pre><code>def record_usage(self, model: str, input_units: int, output_units: int):\n    \"\"\"Records a successful API call and its unit usage.\"\"\"\n    if model not in self._usage_stats:\n        self._usage_stats[model] = ModelUsageStats()\n\n    stats = self._usage_stats[model]\n    stats.calls += 1\n    stats.prompt_tokens += input_units # Map input_units -&gt; prompt_tokens\n    stats.completion_tokens += output_units # Map output_units -&gt; completion_tokens\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiCostTracker.get_total_cost","title":"<code>get_total_cost() -&gt; float</code>","text":"<p>Calculates the estimated total cost based on recorded usage and known model costs per million units.</p> Source code in <code>lean_automator/llm_call.py</code> <pre><code>def get_total_cost(self) -&gt; float:\n    \"\"\"Calculates the estimated total cost based on recorded usage and known model costs per million units.\"\"\"\n    total_cost = 0.0\n    for model, stats in self._usage_stats.items():\n        if model in self._model_costs:\n            costs = self._model_costs[model]\n            total_cost += (stats.prompt_tokens / 1_000_000.0) * costs.input_cost_per_million_units + \\\n                          (stats.completion_tokens / 1_000_000.0) * costs.output_cost_per_million_units\n        else:\n            warnings.warn(f\"Cost information missing for model '{model}'. Usage for this model is not included in total cost.\")\n    return total_cost\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiCostTracker.get_summary","title":"<code>get_summary() -&gt; Dict[str, Any]</code>","text":"<p>Returns a summary dictionary of usage and estimated costs.</p> Source code in <code>lean_automator/llm_call.py</code> <pre><code>def get_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns a summary dictionary of usage and estimated costs.\"\"\"\n    total_estimated_cost = self.get_total_cost() # Ensure warnings are potentially triggered\n    summary: Dict[str, Any] = {\n        \"total_estimated_cost\": total_estimated_cost,\n        \"usage_by_model\": {}\n    }\n    for model, stats in self._usage_stats.items():\n        model_summary = {\n            \"calls\": stats.calls,\n            \"input_units\": stats.prompt_tokens,\n            \"output_units\": stats.completion_tokens, # Usually 0 for embeddings\n            \"estimated_cost\": 0.0\n        }\n        if model in self._model_costs:\n            costs = self._model_costs[model]\n            model_summary[\"estimated_cost\"] = (stats.prompt_tokens / 1_000_000.0) * costs.input_cost_per_million_units + \\\n                                              (stats.completion_tokens / 1_000_000.0) * costs.output_cost_per_million_units\n        else:\n             model_summary[\"estimated_cost\"] = \"Unknown (cost data missing)\"\n\n        summary[\"usage_by_model\"][model] = model_summary\n    return summary\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiClient","title":"<code>GeminiClient</code>","text":"<p>Client for interacting with Google's Gemini API (Generation and Embedding), including async calls, retries, and cost tracking integration.</p> Source code in <code>lean_automator/llm_call.py</code> <pre><code>class GeminiClient:\n    \"\"\"\n    Client for interacting with Google's Gemini API (Generation and Embedding),\n    including async calls, retries, and cost tracking integration.\n    \"\"\"\n    def __init__(self,\n                 api_key: Optional[str] = None,\n                 default_generation_model: Optional[str] = None,\n                 default_embedding_model: Optional[str] = None,\n                 max_retries: Optional[int] = None,\n                 backoff_factor: Optional[float] = None,\n                 cost_tracker: Optional[GeminiCostTracker] = None,\n                 safety_settings: Optional[list] = DEFAULT_SAFETY_SETTINGS):\n        \"\"\"\n        Initializes the Gemini Client. Reads required config from environment if not passed.\n\n        Args:\n            api_key: Google AI API key. If None, reads from GEMINI_API_KEY env var.\n            default_generation_model: Default Gemini model for generation.\n                                      If None, reads from DEFAULT_GEMINI_MODEL env var.\n            default_embedding_model: Default Gemini model for embeddings.\n                                     If None, reads from DEFAULT_EMBEDDING_MODEL env var or uses fallback.\n                                     Ensures 'models/' prefix if needed for cost tracking consistency.\n            max_retries: Max retry attempts. If None, reads from GEMINI_MAX_RETRIES env var or defaults.\n            backoff_factor: Backoff factor for retries. If None, reads from GEMINI_BACKOFF_FACTOR env var or defaults.\n            cost_tracker: An instance of GeminiCostTracker to record usage (optional).\n            safety_settings: Default Gemini safety settings for generation (optional).\n        \"\"\"\n        if not genai:\n             raise RuntimeError(\"google.generativeai package is required but not found.\")\n\n        # --- Configuration Loading ---\n        self.api_key = api_key or os.getenv('GEMINI_API_KEY')\n        if not self.api_key:\n             raise ValueError(\"Gemini API key is missing. Set via argument or GEMINI_API_KEY environment variable.\")\n\n        self.default_generation_model = default_generation_model or os.getenv('DEFAULT_GEMINI_MODEL')\n        if not self.default_generation_model:\n             raise ValueError(\"Default Gemini generation model is missing. Set via argument or DEFAULT_GEMINI_MODEL environment variable.\")\n\n        _emb_model_name = default_embedding_model or os.getenv('DEFAULT_EMBEDDING_MODEL')\n        if not _emb_model_name:\n             warnings.warn(f\"Default embedding model not set via argument or DEFAULT_EMBEDDING_MODEL env var. Using fallback: {FALLBACK_EMBEDDING_MODEL}\")\n             self.default_embedding_model = FALLBACK_EMBEDDING_MODEL\n        else:\n             # Ensure the model name starts with 'models/' for consistency if it doesn't already\n             # This helps align with cost dictionary keys like \"models/text-embedding-004\"\n             if not _emb_model_name.startswith('models/'):\n                 self.default_embedding_model = f'models/{_emb_model_name}'\n                 warnings.warn(f\"DEFAULT_EMBEDDING_MODEL '{_emb_model_name}' did not start with 'models/'. Using '{self.default_embedding_model}' for consistency.\")\n             else:\n                  self.default_embedding_model = _emb_model_name\n\n        # Max Retries\n        if max_retries is not None:\n            self.max_retries = max_retries\n        else:\n            try:\n                _retries_str = os.getenv('GEMINI_MAX_RETRIES')\n                self.max_retries = int(_retries_str) if _retries_str is not None else FALLBACK_MAX_RETRIES\n            except (ValueError, TypeError):\n                warnings.warn(f\"Invalid GEMINI_MAX_RETRIES value '{os.getenv('GEMINI_MAX_RETRIES')}'. Using default {FALLBACK_MAX_RETRIES}.\")\n                self.max_retries = FALLBACK_MAX_RETRIES\n        self.max_retries = max(0, self.max_retries) # Ensure non-negative\n\n        # Backoff Factor\n        if backoff_factor is not None:\n            self.backoff_factor = backoff_factor\n        else:\n            try:\n                 _backoff_str = os.getenv('GEMINI_BACKOFF_FACTOR')\n                 self.backoff_factor = float(_backoff_str) if _backoff_str is not None else FALLBACK_BACKOFF_FACTOR\n            except (ValueError, TypeError):\n                 warnings.warn(f\"Invalid GEMINI_BACKOFF_FACTOR value '{os.getenv('GEMINI_BACKOFF_FACTOR')}'. Using default {FALLBACK_BACKOFF_FACTOR}.\")\n                 self.backoff_factor = FALLBACK_BACKOFF_FACTOR\n        self.backoff_factor = max(0.0, self.backoff_factor) # Ensure non-negative\n\n        # --- Initialization ---\n        self.cost_tracker = cost_tracker if cost_tracker is not None else GeminiCostTracker()\n        self.safety_settings = safety_settings # Used by generate method\n\n        try:\n            genai.configure(api_key=self.api_key)\n        except Exception as e:\n             raise RuntimeError(f\"Failed to configure Google GenAI client: {e}\") from e\n\n    # --- Private Helper for Retries ---\n    async def _execute_with_retry(self, api_call_func, *args, _model_name_for_log='unknown_model', **kwargs):\n        \"\"\" Executes an async API call with retry logic. \"\"\"\n        final_error: Optional[Exception] = None\n        model_name = _model_name_for_log\n\n        total_attempts = self.max_retries + 1\n        for attempt in range(total_attempts):\n            try:\n                # Use asyncio.to_thread to run the synchronous SDK call in a separate thread\n                response = await asyncio.to_thread(api_call_func, *args, **kwargs)\n                return response # Success\n\n            except google_api_exceptions.ResourceExhausted as e:\n                # Specific handling for rate limits / quota errors - retryable\n                final_error = e\n                warnings.warn(f\"API Quota/Rate Limit Error for {model_name} on attempt {attempt + 1}/{total_attempts}: {e}\")\n                # Continue to retry logic below\n\n            except google_api_exceptions.GoogleAPIError as e:\n                 # Catch other Google API errors (e.g., server errors, bad requests)\n                 final_error = e\n                 # Decide if retryable based on status code maybe? For now, retry most.\n                 # 4xx errors are typically not retryable (Bad Request, Not Found, Invalid Argument)\n                 if 400 &lt;= getattr(e, 'code', 0) &lt; 500:\n                      warnings.warn(f\"API Client Error (4xx) for {model_name} on attempt {attempt + 1}/{total_attempts}: {e}. Not retrying.\")\n                      break # Don't retry client errors\n                 else: # Retry server errors (5xx) or unknown API errors\n                     warnings.warn(f\"API Server/Unknown Error for {model_name} on attempt {attempt + 1}/{total_attempts}: {e}\")\n                 # Continue to retry logic below\n\n            except Exception as e:\n                # Catch broader exceptions (network issues, unexpected errors)\n                final_error = e\n                warnings.warn(f\"Unexpected Error during API call for {model_name} on attempt {attempt + 1}/{total_attempts}: {e}\")\n                # Continue to retry logic below\n\n\n            # --- Retry Logic ---\n            if attempt &lt; self.max_retries:\n                sleep_time = self.backoff_factor * (2 ** attempt)\n                retries_remaining = self.max_retries - attempt\n                warnings.warn(\n                    f\"Retrying API call for {model_name} in {sleep_time:.2f} seconds... ({retries_remaining} retries remaining)\"\n                )\n                await asyncio.sleep(sleep_time)\n            else:\n                # This was the final attempt\n                warnings.warn(f\"API call for {model_name} failed on the final attempt ({attempt + 1}/{total_attempts}).\")\n                break # Exit loop after final attempt\n\n        # If loop finished without returning, raise the last captured error\n        raise final_error if final_error is not None else Exception(f\"Unknown error during API call to {model_name} after {total_attempts} attempts\")\n\n\n    # --- Public API Methods ---\n\n    async def generate(self,\n                       prompt: str,\n                       *,\n                       model: Optional[str] = None,\n                       system_prompt: Optional[str] = None, # Note: system_instruction is arg name\n                       generation_config_override: Optional[Dict[str, Any]] = None,\n                       safety_settings_override: Optional[list] = None\n                       ) -&gt; str:\n        \"\"\"\n        Generates content using the specified Gemini model, with retry logic.\n\n        Args:\n            prompt: The main user prompt.\n            model: Specific Gemini model name. Defaults to client's default_generation_model.\n            system_prompt: Optional system instruction.\n            generation_config_override: Optional dictionary to override generation config.\n            safety_settings_override: Optional list to override safety settings.\n\n        Returns:\n            The generated text content.\n\n        Raises:\n            Exception: If the API call fails after all retry attempts.\n            ValueError: If the response is invalid (blocked, empty) or model init fails.\n        \"\"\"\n        effective_model = model or self.default_generation_model\n        gen_config = genai_types.GenerationConfig(**generation_config_override) if generation_config_override else None\n        safety_settings = safety_settings_override if safety_settings_override is not None else self.safety_settings\n\n        try:\n             # Initialize model instance - validation happens here\n             # Note: System instruction should be passed here if supported by the specific model version/SDK\n             model_instance = genai.GenerativeModel(\n                 effective_model,\n                 system_instruction=system_prompt # Pass system prompt here\n             )\n        except Exception as e:\n             # Catch errors during model initialization (e.g., invalid name)\n             raise ValueError(f\"Failed to initialize generative model '{effective_model}'. Check model name. Error: {e}\") from e\n\n        contents = [{'role': 'user', 'parts': [prompt]}]\n        # System prompt is handled by model instance initialization now\n\n        try:\n            # Use the retry helper\n            api_kwargs = { \n                'contents': contents,\n                'generation_config': gen_config,\n                'safety_settings': safety_settings,\n            }\n            response = await self._execute_with_retry(\n                model_instance.generate_content,\n                _model_name_for_log=effective_model, # Log arg\n                **api_kwargs # API args\n            )\n\n            # --- Process Response ---\n            generated_text = None\n            prompt_tokens = 0\n            completion_tokens = 0\n            usage_metadata = getattr(response, 'usage_metadata', None)\n\n            try:\n                # Attempt to access generated text safely\n                # response.text can raise ValueError if content is blocked\n                generated_text = response.text\n            except ValueError as e:\n                # Handle cases where accessing .text fails (e.g., blocked content)\n                 block_reason = \"Unknown\"\n                 try:\n                     if response.prompt_feedback and response.prompt_feedback.block_reason:\n                         block_reason = getattr(response.prompt_feedback.block_reason, 'name', str(response.prompt_feedback.block_reason))\n                 except AttributeError: pass\n                 raise ValueError(f\"API call failed for {effective_model}: Content blocked or invalid. Reason: {block_reason}. Original Error: {e}\") from e\n            except AttributeError:\n                 # If .text attribute doesn't exist (shouldn't happen with valid response)\n                 pass # We handle None generated_text below\n\n            # Fallback text extraction if needed (though response.text should usually work or raise error)\n            if generated_text is None:\n                 try:\n                     if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:\n                         generated_text = \"\".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))\n                     if not generated_text or not generated_text.strip(): # Ensure we got some text\n                         raise ValueError(f\"API call failed for {effective_model}: Received no valid text content in response.\")\n                 except (AttributeError, IndexError, ValueError) as text_extract_err:\n                     raise ValueError(f\"API call failed for {effective_model}: Could not extract text from response structure.\") from text_extract_err\n\n            # --- Token Counting &amp; Cost Tracking ---\n            if usage_metadata:\n                try:\n                    # Use names consistent with google-genai v0.3+\n                    prompt_tokens = getattr(usage_metadata, 'prompt_token_count', 0)\n                    completion_tokens = getattr(usage_metadata, 'candidates_token_count', 0) # Sum of tokens across candidates\n                    prompt_tokens = int(prompt_tokens) if prompt_tokens is not None else 0\n                    completion_tokens = int(completion_tokens) if completion_tokens is not None else 0\n                except (AttributeError, ValueError, TypeError) as e:\n                    warnings.warn(f\"Error accessing token counts from usage metadata for {effective_model}: {e}. Cost tracking may be inaccurate.\")\n                    prompt_tokens = 0\n                    completion_tokens = 0\n\n                if self.cost_tracker:\n                    # Record usage: Map prompt_tokens -&gt; input_units, completion_tokens -&gt; output_units\n                    self.cost_tracker.record_usage(effective_model, prompt_tokens, completion_tokens)\n            else:\n                warnings.warn(f\"Response object for model '{effective_model}' lacks 'usage_metadata'. Cost tracking may be inaccurate.\")\n\n            return generated_text\n\n        except ValueError as ve:\n             # Catch ValueErrors raised during response processing (blocking, empty content)\n             # These are definitive failures, don't wrap further\n             raise ve\n        except Exception as e:\n             # Catch errors from _execute_with_retry (after all retries failed)\n             raise Exception(f\"API call to generation model '{effective_model}' failed after multiple retries.\") from e\n\n\n    async def embed_content(self,\n                            contents: Union[str, List[str]],\n                            task_type: str,\n                            *,\n                            model: Optional[str] = None,\n                            title: Optional[str] = None, # Optional for RETRIEVAL_DOCUMENT\n                            output_dimensionality: Optional[int] = None # Optional truncation\n                            ) -&gt; List[List[float]]:\n        \"\"\"\n        Generates embeddings for the given text content(s) using the specified Gemini model,\n        with retry logic.\n\n        Args:\n            contents: A single string or a list of strings to embed.\n            task_type: The task type for the embedding (e.g., \"RETRIEVAL_DOCUMENT\",\n                       \"RETRIEVAL_QUERY\", \"SEMANTIC_SIMILARITY\"). See Google AI docs.\n            model: Specific Gemini embedding model name. Defaults to client's default_embedding_model.\n                   Should typically start with 'models/' like 'models/text-embedding-004'.\n            title: An optional title when task_type=\"RETRIEVAL_DOCUMENT\".\n            output_dimensionality: Optional dimension to truncate the output embedding to.\n\n        Returns:\n            A list of embedding vectors (list of lists of floats). If a single string\n            was input, the outer list will contain one vector.\n\n        Raises:\n            Exception: If the API call fails after all retry attempts.\n            ValueError: If the response is invalid or parameters are incorrect.\n            TypeError: If input types are wrong.\n        \"\"\"\n        effective_model = model or self.default_embedding_model\n        if not effective_model.startswith(\"models/\"):\n             # Enforce 'models/' prefix based on how cost dict and API often work\n             warnings.warn(f\"Embedding model name '{effective_model}' should ideally start with 'models/'. Attempting call anyway.\")\n             # Consider adding prefix here if API consistently fails without it:\n             # effective_model = f'models/{effective_model}'\n\n        # Validate contents type\n        if not isinstance(contents, (str, list)):\n             raise TypeError(\"contents must be a string or a list of strings.\")\n        if isinstance(contents, list) and not all(isinstance(item, str) for item in contents):\n             raise TypeError(\"If contents is a list, all items must be strings.\")\n\n        # Prepare arguments for genai.embed_content\n        embed_args = {\n             \"model\": effective_model,\n             \"content\": contents, # Pass str or list directly\n             \"task_type\": task_type,\n        }\n        if title is not None and task_type == \"RETRIEVAL_DOCUMENT\":\n            embed_args[\"title\"] = title\n        elif title is not None:\n             warnings.warn(f\"Ignoring 'title' argument as task_type is '{task_type}', not 'RETRIEVAL_DOCUMENT'.\")\n\n        if output_dimensionality is not None:\n             embed_args[\"output_dimensionality\"] = output_dimensionality\n\n        try:\n            # Use the retry helper\n            response_dict = await self._execute_with_retry(\n                genai.embed_content,\n                _model_name_for_log=effective_model, # Log arg\n                **embed_args # API args\n            )\n\n            # --- Process Response ---\n            embeddings: List[List[float]] = []\n            if 'embedding' in response_dict: # Response for single string input\n                 if isinstance(response_dict['embedding'], list):\n                    embeddings = [response_dict['embedding']]\n                 else:\n                      raise ValueError(f\"Invalid embedding format received for single input in {effective_model}.\")\n            elif 'embeddings' in response_dict: # Response for list input\n                 if isinstance(response_dict['embeddings'], list) and all(isinstance(e, list) for e in response_dict['embeddings']):\n                      embeddings = response_dict['embeddings']\n                 else:\n                      raise ValueError(f\"Invalid embedding format received for list input in {effective_model}.\")\n            else:\n                # Should not happen if API call succeeded without error\n                raise ValueError(f\"API call for {effective_model} succeeded but response missing 'embedding' or 'embeddings' key.\")\n\n            # --- Token Counting &amp; Cost Tracking (Attempt) ---\n            # NOTE: Usage metadata is often NOT included in embedding responses.\n            # Add specific check if response structure is known, otherwise assume unavailable.\n            usage_metadata = response_dict.get('usage_metadata', None) # Assuming it might be dict\n            input_units = 0\n            output_units = 0 # Always 0 for embeddings\n\n            if usage_metadata and isinstance(usage_metadata, dict):\n                 # Example: Adjust key based on actual response if available\n                 # E.g., 'total_token_count', 'prompt_token_count'\n                 token_key = 'total_token_count' # Replace with actual key if known\n                 if token_key in usage_metadata:\n                     try:\n                         input_units = int(usage_metadata[token_key])\n                     except (ValueError, TypeError):\n                         warnings.warn(f\"Could not parse '{token_key}' from usage metadata for {effective_model}.\")\n                         input_units = 0\n                 else:\n                      warnings.warn(f\"Expected key '{token_key}' not found in usage metadata for {effective_model}.\")\n\n                 if input_units &gt; 0 and self.cost_tracker:\n                      self.cost_tracker.record_usage(effective_model, input_units, output_units)\n                 elif input_units == 0:\n                      # Only warn if we expected metadata but couldn't parse/find tokens\n                      warnings.warn(f\"Could not determine input units from usage metadata for {effective_model}. Cost tracking may be inaccurate.\")\n\n            else:\n                 # This is the expected path if usage metadata is typically absent\n                 warnings.warn(f\"Usage metadata not found in response for embedding model '{effective_model}'. Cost tracking skipped for this call.\")\n\n            return embeddings\n\n        except ValueError as ve:\n            # Catch ValueErrors raised during response processing\n            raise ve\n        except Exception as e:\n            # Catch errors from _execute_with_retry (after all retries failed)\n            raise Exception(f\"API call to embedding model '{effective_model}' failed after multiple retries.\") from e\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiClient-functions","title":"Functions","text":""},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiClient.__init__","title":"<code>__init__(api_key: Optional[str] = None, default_generation_model: Optional[str] = None, default_embedding_model: Optional[str] = None, max_retries: Optional[int] = None, backoff_factor: Optional[float] = None, cost_tracker: Optional[GeminiCostTracker] = None, safety_settings: Optional[list] = DEFAULT_SAFETY_SETTINGS)</code>","text":"<p>Initializes the Gemini Client. Reads required config from environment if not passed.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>Google AI API key. If None, reads from GEMINI_API_KEY env var.</p> <code>None</code> <code>default_generation_model</code> <code>Optional[str]</code> <p>Default Gemini model for generation.                       If None, reads from DEFAULT_GEMINI_MODEL env var.</p> <code>None</code> <code>default_embedding_model</code> <code>Optional[str]</code> <p>Default Gemini model for embeddings.                      If None, reads from DEFAULT_EMBEDDING_MODEL env var or uses fallback.                      Ensures 'models/' prefix if needed for cost tracking consistency.</p> <code>None</code> <code>max_retries</code> <code>Optional[int]</code> <p>Max retry attempts. If None, reads from GEMINI_MAX_RETRIES env var or defaults.</p> <code>None</code> <code>backoff_factor</code> <code>Optional[float]</code> <p>Backoff factor for retries. If None, reads from GEMINI_BACKOFF_FACTOR env var or defaults.</p> <code>None</code> <code>cost_tracker</code> <code>Optional[GeminiCostTracker]</code> <p>An instance of GeminiCostTracker to record usage (optional).</p> <code>None</code> <code>safety_settings</code> <code>Optional[list]</code> <p>Default Gemini safety settings for generation (optional).</p> <code>DEFAULT_SAFETY_SETTINGS</code> Source code in <code>lean_automator/llm_call.py</code> <pre><code>def __init__(self,\n             api_key: Optional[str] = None,\n             default_generation_model: Optional[str] = None,\n             default_embedding_model: Optional[str] = None,\n             max_retries: Optional[int] = None,\n             backoff_factor: Optional[float] = None,\n             cost_tracker: Optional[GeminiCostTracker] = None,\n             safety_settings: Optional[list] = DEFAULT_SAFETY_SETTINGS):\n    \"\"\"\n    Initializes the Gemini Client. Reads required config from environment if not passed.\n\n    Args:\n        api_key: Google AI API key. If None, reads from GEMINI_API_KEY env var.\n        default_generation_model: Default Gemini model for generation.\n                                  If None, reads from DEFAULT_GEMINI_MODEL env var.\n        default_embedding_model: Default Gemini model for embeddings.\n                                 If None, reads from DEFAULT_EMBEDDING_MODEL env var or uses fallback.\n                                 Ensures 'models/' prefix if needed for cost tracking consistency.\n        max_retries: Max retry attempts. If None, reads from GEMINI_MAX_RETRIES env var or defaults.\n        backoff_factor: Backoff factor for retries. If None, reads from GEMINI_BACKOFF_FACTOR env var or defaults.\n        cost_tracker: An instance of GeminiCostTracker to record usage (optional).\n        safety_settings: Default Gemini safety settings for generation (optional).\n    \"\"\"\n    if not genai:\n         raise RuntimeError(\"google.generativeai package is required but not found.\")\n\n    # --- Configuration Loading ---\n    self.api_key = api_key or os.getenv('GEMINI_API_KEY')\n    if not self.api_key:\n         raise ValueError(\"Gemini API key is missing. Set via argument or GEMINI_API_KEY environment variable.\")\n\n    self.default_generation_model = default_generation_model or os.getenv('DEFAULT_GEMINI_MODEL')\n    if not self.default_generation_model:\n         raise ValueError(\"Default Gemini generation model is missing. Set via argument or DEFAULT_GEMINI_MODEL environment variable.\")\n\n    _emb_model_name = default_embedding_model or os.getenv('DEFAULT_EMBEDDING_MODEL')\n    if not _emb_model_name:\n         warnings.warn(f\"Default embedding model not set via argument or DEFAULT_EMBEDDING_MODEL env var. Using fallback: {FALLBACK_EMBEDDING_MODEL}\")\n         self.default_embedding_model = FALLBACK_EMBEDDING_MODEL\n    else:\n         # Ensure the model name starts with 'models/' for consistency if it doesn't already\n         # This helps align with cost dictionary keys like \"models/text-embedding-004\"\n         if not _emb_model_name.startswith('models/'):\n             self.default_embedding_model = f'models/{_emb_model_name}'\n             warnings.warn(f\"DEFAULT_EMBEDDING_MODEL '{_emb_model_name}' did not start with 'models/'. Using '{self.default_embedding_model}' for consistency.\")\n         else:\n              self.default_embedding_model = _emb_model_name\n\n    # Max Retries\n    if max_retries is not None:\n        self.max_retries = max_retries\n    else:\n        try:\n            _retries_str = os.getenv('GEMINI_MAX_RETRIES')\n            self.max_retries = int(_retries_str) if _retries_str is not None else FALLBACK_MAX_RETRIES\n        except (ValueError, TypeError):\n            warnings.warn(f\"Invalid GEMINI_MAX_RETRIES value '{os.getenv('GEMINI_MAX_RETRIES')}'. Using default {FALLBACK_MAX_RETRIES}.\")\n            self.max_retries = FALLBACK_MAX_RETRIES\n    self.max_retries = max(0, self.max_retries) # Ensure non-negative\n\n    # Backoff Factor\n    if backoff_factor is not None:\n        self.backoff_factor = backoff_factor\n    else:\n        try:\n             _backoff_str = os.getenv('GEMINI_BACKOFF_FACTOR')\n             self.backoff_factor = float(_backoff_str) if _backoff_str is not None else FALLBACK_BACKOFF_FACTOR\n        except (ValueError, TypeError):\n             warnings.warn(f\"Invalid GEMINI_BACKOFF_FACTOR value '{os.getenv('GEMINI_BACKOFF_FACTOR')}'. Using default {FALLBACK_BACKOFF_FACTOR}.\")\n             self.backoff_factor = FALLBACK_BACKOFF_FACTOR\n    self.backoff_factor = max(0.0, self.backoff_factor) # Ensure non-negative\n\n    # --- Initialization ---\n    self.cost_tracker = cost_tracker if cost_tracker is not None else GeminiCostTracker()\n    self.safety_settings = safety_settings # Used by generate method\n\n    try:\n        genai.configure(api_key=self.api_key)\n    except Exception as e:\n         raise RuntimeError(f\"Failed to configure Google GenAI client: {e}\") from e\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiClient.generate","title":"<code>generate(prompt: str, *, model: Optional[str] = None, system_prompt: Optional[str] = None, generation_config_override: Optional[Dict[str, Any]] = None, safety_settings_override: Optional[list] = None) -&gt; str</code>  <code>async</code>","text":"<p>Generates content using the specified Gemini model, with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The main user prompt.</p> required <code>model</code> <code>Optional[str]</code> <p>Specific Gemini model name. Defaults to client's default_generation_model.</p> <code>None</code> <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system instruction.</p> <code>None</code> <code>generation_config_override</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary to override generation config.</p> <code>None</code> <code>safety_settings_override</code> <code>Optional[list]</code> <p>Optional list to override safety settings.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated text content.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the API call fails after all retry attempts.</p> <code>ValueError</code> <p>If the response is invalid (blocked, empty) or model init fails.</p> Source code in <code>lean_automator/llm_call.py</code> <pre><code>async def generate(self,\n                   prompt: str,\n                   *,\n                   model: Optional[str] = None,\n                   system_prompt: Optional[str] = None, # Note: system_instruction is arg name\n                   generation_config_override: Optional[Dict[str, Any]] = None,\n                   safety_settings_override: Optional[list] = None\n                   ) -&gt; str:\n    \"\"\"\n    Generates content using the specified Gemini model, with retry logic.\n\n    Args:\n        prompt: The main user prompt.\n        model: Specific Gemini model name. Defaults to client's default_generation_model.\n        system_prompt: Optional system instruction.\n        generation_config_override: Optional dictionary to override generation config.\n        safety_settings_override: Optional list to override safety settings.\n\n    Returns:\n        The generated text content.\n\n    Raises:\n        Exception: If the API call fails after all retry attempts.\n        ValueError: If the response is invalid (blocked, empty) or model init fails.\n    \"\"\"\n    effective_model = model or self.default_generation_model\n    gen_config = genai_types.GenerationConfig(**generation_config_override) if generation_config_override else None\n    safety_settings = safety_settings_override if safety_settings_override is not None else self.safety_settings\n\n    try:\n         # Initialize model instance - validation happens here\n         # Note: System instruction should be passed here if supported by the specific model version/SDK\n         model_instance = genai.GenerativeModel(\n             effective_model,\n             system_instruction=system_prompt # Pass system prompt here\n         )\n    except Exception as e:\n         # Catch errors during model initialization (e.g., invalid name)\n         raise ValueError(f\"Failed to initialize generative model '{effective_model}'. Check model name. Error: {e}\") from e\n\n    contents = [{'role': 'user', 'parts': [prompt]}]\n    # System prompt is handled by model instance initialization now\n\n    try:\n        # Use the retry helper\n        api_kwargs = { \n            'contents': contents,\n            'generation_config': gen_config,\n            'safety_settings': safety_settings,\n        }\n        response = await self._execute_with_retry(\n            model_instance.generate_content,\n            _model_name_for_log=effective_model, # Log arg\n            **api_kwargs # API args\n        )\n\n        # --- Process Response ---\n        generated_text = None\n        prompt_tokens = 0\n        completion_tokens = 0\n        usage_metadata = getattr(response, 'usage_metadata', None)\n\n        try:\n            # Attempt to access generated text safely\n            # response.text can raise ValueError if content is blocked\n            generated_text = response.text\n        except ValueError as e:\n            # Handle cases where accessing .text fails (e.g., blocked content)\n             block_reason = \"Unknown\"\n             try:\n                 if response.prompt_feedback and response.prompt_feedback.block_reason:\n                     block_reason = getattr(response.prompt_feedback.block_reason, 'name', str(response.prompt_feedback.block_reason))\n             except AttributeError: pass\n             raise ValueError(f\"API call failed for {effective_model}: Content blocked or invalid. Reason: {block_reason}. Original Error: {e}\") from e\n        except AttributeError:\n             # If .text attribute doesn't exist (shouldn't happen with valid response)\n             pass # We handle None generated_text below\n\n        # Fallback text extraction if needed (though response.text should usually work or raise error)\n        if generated_text is None:\n             try:\n                 if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:\n                     generated_text = \"\".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))\n                 if not generated_text or not generated_text.strip(): # Ensure we got some text\n                     raise ValueError(f\"API call failed for {effective_model}: Received no valid text content in response.\")\n             except (AttributeError, IndexError, ValueError) as text_extract_err:\n                 raise ValueError(f\"API call failed for {effective_model}: Could not extract text from response structure.\") from text_extract_err\n\n        # --- Token Counting &amp; Cost Tracking ---\n        if usage_metadata:\n            try:\n                # Use names consistent with google-genai v0.3+\n                prompt_tokens = getattr(usage_metadata, 'prompt_token_count', 0)\n                completion_tokens = getattr(usage_metadata, 'candidates_token_count', 0) # Sum of tokens across candidates\n                prompt_tokens = int(prompt_tokens) if prompt_tokens is not None else 0\n                completion_tokens = int(completion_tokens) if completion_tokens is not None else 0\n            except (AttributeError, ValueError, TypeError) as e:\n                warnings.warn(f\"Error accessing token counts from usage metadata for {effective_model}: {e}. Cost tracking may be inaccurate.\")\n                prompt_tokens = 0\n                completion_tokens = 0\n\n            if self.cost_tracker:\n                # Record usage: Map prompt_tokens -&gt; input_units, completion_tokens -&gt; output_units\n                self.cost_tracker.record_usage(effective_model, prompt_tokens, completion_tokens)\n        else:\n            warnings.warn(f\"Response object for model '{effective_model}' lacks 'usage_metadata'. Cost tracking may be inaccurate.\")\n\n        return generated_text\n\n    except ValueError as ve:\n         # Catch ValueErrors raised during response processing (blocking, empty content)\n         # These are definitive failures, don't wrap further\n         raise ve\n    except Exception as e:\n         # Catch errors from _execute_with_retry (after all retries failed)\n         raise Exception(f\"API call to generation model '{effective_model}' failed after multiple retries.\") from e\n</code></pre>"},{"location":"reference/llm_call/#lean_automator.llm_call.GeminiClient.embed_content","title":"<code>embed_content(contents: Union[str, List[str]], task_type: str, *, model: Optional[str] = None, title: Optional[str] = None, output_dimensionality: Optional[int] = None) -&gt; List[List[float]]</code>  <code>async</code>","text":"<p>Generates embeddings for the given text content(s) using the specified Gemini model, with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>contents</code> <code>Union[str, List[str]]</code> <p>A single string or a list of strings to embed.</p> required <code>task_type</code> <code>str</code> <p>The task type for the embedding (e.g., \"RETRIEVAL_DOCUMENT\",        \"RETRIEVAL_QUERY\", \"SEMANTIC_SIMILARITY\"). See Google AI docs.</p> required <code>model</code> <code>Optional[str]</code> <p>Specific Gemini embedding model name. Defaults to client's default_embedding_model.    Should typically start with 'models/' like 'models/text-embedding-004'.</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>An optional title when task_type=\"RETRIEVAL_DOCUMENT\".</p> <code>None</code> <code>output_dimensionality</code> <code>Optional[int]</code> <p>Optional dimension to truncate the output embedding to.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>A list of embedding vectors (list of lists of floats). If a single string</p> <code>List[List[float]]</code> <p>was input, the outer list will contain one vector.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the API call fails after all retry attempts.</p> <code>ValueError</code> <p>If the response is invalid or parameters are incorrect.</p> <code>TypeError</code> <p>If input types are wrong.</p> Source code in <code>lean_automator/llm_call.py</code> <pre><code>async def embed_content(self,\n                        contents: Union[str, List[str]],\n                        task_type: str,\n                        *,\n                        model: Optional[str] = None,\n                        title: Optional[str] = None, # Optional for RETRIEVAL_DOCUMENT\n                        output_dimensionality: Optional[int] = None # Optional truncation\n                        ) -&gt; List[List[float]]:\n    \"\"\"\n    Generates embeddings for the given text content(s) using the specified Gemini model,\n    with retry logic.\n\n    Args:\n        contents: A single string or a list of strings to embed.\n        task_type: The task type for the embedding (e.g., \"RETRIEVAL_DOCUMENT\",\n                   \"RETRIEVAL_QUERY\", \"SEMANTIC_SIMILARITY\"). See Google AI docs.\n        model: Specific Gemini embedding model name. Defaults to client's default_embedding_model.\n               Should typically start with 'models/' like 'models/text-embedding-004'.\n        title: An optional title when task_type=\"RETRIEVAL_DOCUMENT\".\n        output_dimensionality: Optional dimension to truncate the output embedding to.\n\n    Returns:\n        A list of embedding vectors (list of lists of floats). If a single string\n        was input, the outer list will contain one vector.\n\n    Raises:\n        Exception: If the API call fails after all retry attempts.\n        ValueError: If the response is invalid or parameters are incorrect.\n        TypeError: If input types are wrong.\n    \"\"\"\n    effective_model = model or self.default_embedding_model\n    if not effective_model.startswith(\"models/\"):\n         # Enforce 'models/' prefix based on how cost dict and API often work\n         warnings.warn(f\"Embedding model name '{effective_model}' should ideally start with 'models/'. Attempting call anyway.\")\n         # Consider adding prefix here if API consistently fails without it:\n         # effective_model = f'models/{effective_model}'\n\n    # Validate contents type\n    if not isinstance(contents, (str, list)):\n         raise TypeError(\"contents must be a string or a list of strings.\")\n    if isinstance(contents, list) and not all(isinstance(item, str) for item in contents):\n         raise TypeError(\"If contents is a list, all items must be strings.\")\n\n    # Prepare arguments for genai.embed_content\n    embed_args = {\n         \"model\": effective_model,\n         \"content\": contents, # Pass str or list directly\n         \"task_type\": task_type,\n    }\n    if title is not None and task_type == \"RETRIEVAL_DOCUMENT\":\n        embed_args[\"title\"] = title\n    elif title is not None:\n         warnings.warn(f\"Ignoring 'title' argument as task_type is '{task_type}', not 'RETRIEVAL_DOCUMENT'.\")\n\n    if output_dimensionality is not None:\n         embed_args[\"output_dimensionality\"] = output_dimensionality\n\n    try:\n        # Use the retry helper\n        response_dict = await self._execute_with_retry(\n            genai.embed_content,\n            _model_name_for_log=effective_model, # Log arg\n            **embed_args # API args\n        )\n\n        # --- Process Response ---\n        embeddings: List[List[float]] = []\n        if 'embedding' in response_dict: # Response for single string input\n             if isinstance(response_dict['embedding'], list):\n                embeddings = [response_dict['embedding']]\n             else:\n                  raise ValueError(f\"Invalid embedding format received for single input in {effective_model}.\")\n        elif 'embeddings' in response_dict: # Response for list input\n             if isinstance(response_dict['embeddings'], list) and all(isinstance(e, list) for e in response_dict['embeddings']):\n                  embeddings = response_dict['embeddings']\n             else:\n                  raise ValueError(f\"Invalid embedding format received for list input in {effective_model}.\")\n        else:\n            # Should not happen if API call succeeded without error\n            raise ValueError(f\"API call for {effective_model} succeeded but response missing 'embedding' or 'embeddings' key.\")\n\n        # --- Token Counting &amp; Cost Tracking (Attempt) ---\n        # NOTE: Usage metadata is often NOT included in embedding responses.\n        # Add specific check if response structure is known, otherwise assume unavailable.\n        usage_metadata = response_dict.get('usage_metadata', None) # Assuming it might be dict\n        input_units = 0\n        output_units = 0 # Always 0 for embeddings\n\n        if usage_metadata and isinstance(usage_metadata, dict):\n             # Example: Adjust key based on actual response if available\n             # E.g., 'total_token_count', 'prompt_token_count'\n             token_key = 'total_token_count' # Replace with actual key if known\n             if token_key in usage_metadata:\n                 try:\n                     input_units = int(usage_metadata[token_key])\n                 except (ValueError, TypeError):\n                     warnings.warn(f\"Could not parse '{token_key}' from usage metadata for {effective_model}.\")\n                     input_units = 0\n             else:\n                  warnings.warn(f\"Expected key '{token_key}' not found in usage metadata for {effective_model}.\")\n\n             if input_units &gt; 0 and self.cost_tracker:\n                  self.cost_tracker.record_usage(effective_model, input_units, output_units)\n             elif input_units == 0:\n                  # Only warn if we expected metadata but couldn't parse/find tokens\n                  warnings.warn(f\"Could not determine input units from usage metadata for {effective_model}. Cost tracking may be inaccurate.\")\n\n        else:\n             # This is the expected path if usage metadata is typically absent\n             warnings.warn(f\"Usage metadata not found in response for embedding model '{effective_model}'. Cost tracking skipped for this call.\")\n\n        return embeddings\n\n    except ValueError as ve:\n        # Catch ValueErrors raised during response processing\n        raise ve\n    except Exception as e:\n        # Catch errors from _execute_with_retry (after all retries failed)\n        raise Exception(f\"API call to embedding model '{effective_model}' failed after multiple retries.\") from e\n</code></pre>"}]}